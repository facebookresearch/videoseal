{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /private/home/pfz/07-segmark/2404-dev\n",
    "\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = \"{:.1e}\".format\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from videoseal.losses.dwt import DWTForward, DWTInverse\n",
    "\n",
    "def imshow(img, pixel_mean=[123.675, 116.28, 103.53], pixel_std=[58.395, 57.12, 57.375]):\n",
    "    img = img * torch.tensor(pixel_std).view(3, 1, 1) + torch.tensor(pixel_mean).view(3, 1, 1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.utils.display import save_vid\n",
    "from videoseal.augmentation import H264, Crop\n",
    "\n",
    "video_files = \"/checkpoint/pfz/projects/videoseal/assets/videos/metamoviegen/metamoviegen_3s.mp4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>forward_time_per_img</th>\n",
       "      <th>interp_time_per_img</th>\n",
       "      <th>norm_time_per_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hidden</td>\n",
       "      <td>2.5e-01</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unet_small</td>\n",
       "      <td>5.3e-01</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unet_small_bw</td>\n",
       "      <td>5.3e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unet_small_bw_zero_init</td>\n",
       "      <td>5.5e-01</td>\n",
       "      <td>1.4e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unet_small_notanh</td>\n",
       "      <td>5.5e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unet</td>\n",
       "      <td>8.2e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unet_small2</td>\n",
       "      <td>6.0e-01</td>\n",
       "      <td>1.4e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unet_bw</td>\n",
       "      <td>8.3e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unet_notanh</td>\n",
       "      <td>8.4e-01</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unet_big</td>\n",
       "      <td>1.4e+00</td>\n",
       "      <td>1.4e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vae_tiny</td>\n",
       "      <td>4.1e-01</td>\n",
       "      <td>1.2e-03</td>\n",
       "      <td>8.5e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vae_small</td>\n",
       "      <td>7.8e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vae_small_yuv</td>\n",
       "      <td>6.9e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vae_small_bw</td>\n",
       "      <td>7.9e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vae_big</td>\n",
       "      <td>1.2e+00</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.3e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hidden</td>\n",
       "      <td>4.4e-01</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hidden_pw</td>\n",
       "      <td>4.6e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hidden_bn</td>\n",
       "      <td>4.3e-01</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hidden_bn_pw</td>\n",
       "      <td>4.5e-01</td>\n",
       "      <td>1.7e-03</td>\n",
       "      <td>1.3e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sam_tiny</td>\n",
       "      <td>7.0e-02</td>\n",
       "      <td>1.7e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sam_tiny_pw</td>\n",
       "      <td>1.3e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sam_small</td>\n",
       "      <td>1.4e-01</td>\n",
       "      <td>1.3e-03</td>\n",
       "      <td>7.8e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sam_small_pw</td>\n",
       "      <td>2.6e-01</td>\n",
       "      <td>1.1e-03</td>\n",
       "      <td>6.6e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sam_base</td>\n",
       "      <td>6.0e-01</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sam_small_conv</td>\n",
       "      <td>9.5e-02</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>3.9e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sam_small_unshuffle</td>\n",
       "      <td>9.3e-02</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>4.4e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sam_small_nearest</td>\n",
       "      <td>1.1e-01</td>\n",
       "      <td>1.3e-03</td>\n",
       "      <td>5.4e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>convnext</td>\n",
       "      <td>1.1e-01</td>\n",
       "      <td>1.6e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>convnext_femto</td>\n",
       "      <td>1.4e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>convnext_pico</td>\n",
       "      <td>1.9e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>convnext_nano</td>\n",
       "      <td>2.4e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.1e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>convnext_tiny</td>\n",
       "      <td>3.6e-01</td>\n",
       "      <td>1.5e-03</td>\n",
       "      <td>1.2e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  forward_time_per_img  interp_time_per_img  \\\n",
       "0                    hidden               2.5e-01              1.6e-03   \n",
       "1                unet_small               5.3e-01              1.6e-03   \n",
       "2             unet_small_bw               5.3e-01              1.5e-03   \n",
       "3   unet_small_bw_zero_init               5.5e-01              1.4e-03   \n",
       "4         unet_small_notanh               5.5e-01              1.5e-03   \n",
       "5                      unet               8.2e-01              1.5e-03   \n",
       "6               unet_small2               6.0e-01              1.4e-03   \n",
       "7                   unet_bw               8.3e-01              1.5e-03   \n",
       "8               unet_notanh               8.4e-01              1.6e-03   \n",
       "9                  unet_big               1.4e+00              1.4e-03   \n",
       "10                 vae_tiny               4.1e-01              1.2e-03   \n",
       "11                vae_small               7.8e-01              1.5e-03   \n",
       "12            vae_small_yuv               6.9e-01              1.5e-03   \n",
       "13             vae_small_bw               7.9e-01              1.5e-03   \n",
       "14                  vae_big               1.2e+00              1.6e-03   \n",
       "15                   hidden               4.4e-01              1.6e-03   \n",
       "16                hidden_pw               4.6e-01              1.5e-03   \n",
       "17                hidden_bn               4.3e-01              1.6e-03   \n",
       "18             hidden_bn_pw               4.5e-01              1.7e-03   \n",
       "19                 sam_tiny               7.0e-02              1.7e-03   \n",
       "20              sam_tiny_pw               1.3e-01              1.5e-03   \n",
       "21                sam_small               1.4e-01              1.3e-03   \n",
       "22             sam_small_pw               2.6e-01              1.1e-03   \n",
       "23                 sam_base               6.0e-01              1.6e-03   \n",
       "24           sam_small_conv               9.5e-02              1.5e-03   \n",
       "25      sam_small_unshuffle               9.3e-02              1.5e-03   \n",
       "26        sam_small_nearest               1.1e-01              1.3e-03   \n",
       "27                 convnext               1.1e-01              1.6e-03   \n",
       "28           convnext_femto               1.4e-01              1.5e-03   \n",
       "29            convnext_pico               1.9e-01              1.5e-03   \n",
       "30            convnext_nano               2.4e-01              1.5e-03   \n",
       "31            convnext_tiny               3.6e-01              1.5e-03   \n",
       "\n",
       "    norm_time_per_img  \n",
       "0             1.2e-03  \n",
       "1             1.2e-03  \n",
       "2             1.2e-03  \n",
       "3             1.2e-03  \n",
       "4             1.1e-03  \n",
       "5             1.2e-03  \n",
       "6             1.1e-03  \n",
       "7             1.1e-03  \n",
       "8             1.2e-03  \n",
       "9             1.2e-03  \n",
       "10            8.5e-04  \n",
       "11            1.2e-03  \n",
       "12            1.2e-03  \n",
       "13            1.2e-03  \n",
       "14            1.3e-03  \n",
       "15            1.2e-03  \n",
       "16            1.1e-03  \n",
       "17            1.1e-03  \n",
       "18            1.3e-03  \n",
       "19            1.2e-03  \n",
       "20            1.1e-03  \n",
       "21            7.8e-04  \n",
       "22            6.6e-04  \n",
       "23            1.2e-03  \n",
       "24            3.9e-04  \n",
       "25            4.4e-04  \n",
       "26            5.4e-04  \n",
       "27            1.1e-03  \n",
       "28            1.1e-03  \n",
       "29            1.1e-03  \n",
       "30            1.1e-03  \n",
       "31            1.2e-03  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"/private/home/pfz/09-videoseal/videoseal-dev/output/speed_results_cpu_512.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "# only keep if \"_per_img\" in column name\n",
    "df = df[[col for col in df.columns if (\"_per_img\" in col) or (\"model\" in col)]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests YUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "default_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "normalize_img = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "unnormalize_img = transforms.Compose([\n",
    "    transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "])\n",
    "\n",
    "def torch_to_np(img_tensor):\n",
    "    img_tensor = unnormalize_img(img_tensor).clamp(0, 1)\n",
    "    img_tensor = img_tensor.squeeze().permute(1, 2, 0).cpu()\n",
    "    return img_tensor.numpy()\n",
    "\n",
    "def rgb_to_yuv(img):\n",
    "    M = torch.tensor([[0.299, 0.587, 0.114],\n",
    "                      [-0.14713, -0.28886, 0.436],\n",
    "                      [0.615, -0.51499, -0.10001]], dtype=torch.float32).to(img.device)\n",
    "    img = img.permute(0, 2, 3, 1)  # b h w c\n",
    "    yuv = torch.matmul(img, M)\n",
    "    yuv = yuv.permute(0, 3, 1, 2)\n",
    "    return yuv\n",
    "\n",
    "def yuv_to_rgb(img):\n",
    "    M = torch.tensor([[1.0, 0.0, 1.13983],\n",
    "                      [1.0, -0.39465, -0.58060],\n",
    "                      [1.0, 2.03211, 0.0]], dtype=torch.float32).to(img.device)\n",
    "    img = img.permute(0, 2, 3, 1)  # b h w c\n",
    "    rgb = torch.matmul(img, M)\n",
    "    rgb = rgb.permute(0, 3, 1, 2)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "# print(os.listdir(\"/private/home/pfz/_images\"))\n",
    "img = \"/private/home/pfz/_images/trex_bike.png\"\n",
    "img = Image.open(img, \"r\").convert(\"RGB\")\n",
    "\n",
    "img_pt = to_tensor(img).unsqueeze(0).repeat(16, 1, 1, 1)\n",
    "plt.imshow(img_pt[0].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "\n",
    "img_yuv = rgb_to_yuv(img_pt)\n",
    "\n",
    "lum = img_yuv.clone()[0]\n",
    "lum = lum[0:1, :, :].repeat(3, 1, 1)\n",
    "blue = img_yuv.clone()[0]\n",
    "blue[0:2, :, :] = 0\n",
    "red = img_yuv.clone()[0]\n",
    "red[1:3, :, :] = 0\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(lum.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(blue.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(red.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(yuv_to_rgb(img_yuv)[0].squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests torch DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# Check the images in the directory\n",
    "print(os.listdir(\"/private/home/pfz/_images\"))\n",
    "\n",
    "# Load the image\n",
    "img_path = \"/private/home/pfz/_images/chao.png\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Convert image to tensor\n",
    "img_pt = to_tensor(img).unsqueeze(0)\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_pooling_2x2_stride_1(input_tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of shape (b, c, h, w)\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor after 2x2 sum pooling with stride 1\n",
    "    \"\"\"\n",
    "    padded_input = F.pad(input_tensor, (1, 0, 1, 0), mode='reflect')\n",
    "    kernel = torch.ones((3, 1, 2, 2), dtype=input_tensor.dtype, device=input_tensor.device)\n",
    "    pooled_output = F.conv2d(padded_input, kernel, stride=1, padding=0, groups=input_tensor.size(1))\n",
    "    return pooled_output\n",
    "\n",
    "def local_variance_2x2_stride_1(input_tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of shape (b, c, h, w)\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor with the local variance computed in each 2x2 window with stride 1\n",
    "    \"\"\"\n",
    "    padded_input = F.pad(input_tensor, (1, 0, 1, 0), mode='reflect')\n",
    "    kernel = torch.ones((3, 1, 2, 2), dtype=input_tensor.dtype, device=input_tensor.device)\n",
    "    sum_2x2 = F.conv2d(padded_input, kernel, stride=1, padding=0, groups=input_tensor.size(1))\n",
    "    sum_of_squares_2x2 = F.conv2d(padded_input ** 2, kernel, stride=1, padding=0, groups=input_tensor.size(1))\n",
    "    mean_2x2 = sum_2x2 / 4.0\n",
    "    mean_of_squares_2x2 = sum_of_squares_2x2 / 4.0\n",
    "    local_variance = mean_of_squares_2x2 - mean_2x2 ** 2\n",
    "    return local_variance\n",
    "\n",
    "def get_I(Yh, k, theta):\n",
    "    # k-th level, theta-th orientation\n",
    "    # levels b c 3 h w -> b c h w\n",
    "    if theta == 0:\n",
    "        return Yh[k][:, :, 0]  # horizontal\n",
    "    elif theta == 1:\n",
    "        return Yh[k][:, :, 1]  # vertical\n",
    "    elif theta == 2:\n",
    "        return Yh[k][:, :, 2]  # diagonal\n",
    "    raise ValueError(\"Invalid k or theta value\")\n",
    "\n",
    "def theta(l, theta):\n",
    "    \"\"\"Compute the weighting factor based on level l and orientation theta.\"\"\"\n",
    "    orientation_factor = 1.414 if theta == 2 else 1\n",
    "    level_factors = {0: 1.00, 1: 0.32, 2: 0.16, 3: 0.10}\n",
    "    return orientation_factor * level_factors[l]\n",
    "\n",
    "class DWTHeatmap(nn.Module):\n",
    "\n",
    "    def __init__(self, level=3):\n",
    "        super(DWTHeatmap, self).__init__()\n",
    "        self.dwt_inv = DWTInverse(wave='db1')\n",
    "        self.dwt = DWTForward(J=level, wave='db1')\n",
    "        self.level = level\n",
    "\n",
    "    def texture_mask(self, shape, Yh, Yl):\n",
    "        num_levels = len(Yh)\n",
    "        bsz = Yh[0].shape[0]\n",
    "        chns = Yh[0].shape[1]\n",
    "        result = torch.zeros(bsz, chns, shape[0], shape[1], device=Yh[0].device)\n",
    "        for k in range(num_levels):\n",
    "            term_k = 1 / (16 ** k)\n",
    "            for theta in range(3):\n",
    "                I_kl_theta = get_I(Yh, k, theta)  # b c h/f w/f\n",
    "                I_term_squared = I_kl_theta ** 2\n",
    "                I_term_squared = sum_pooling_2x2_stride_1(I_term_squared)\n",
    "                I_term_squared = F.interpolate(I_term_squared, size=shape, mode='nearest')\n",
    "                result += term_k * I_term_squared\n",
    "        I_variance = local_variance_2x2_stride_1(Yl)\n",
    "        I_variance = F.interpolate(I_variance, size=shape, mode='nearest')\n",
    "        result *= I_variance\n",
    "        return result\n",
    "\n",
    "    def luminance_mask(self, shape, Yl_norm):\n",
    "        \"\"\"\n",
    "        Compute the local brightness adjustment factor at level l.\n",
    "        Done by upsampling the last approximation level to the current level.\n",
    "        Args:\n",
    "            shape (tuple): The shape of the current level. (H, W)\n",
    "            Yl (torch.Tensor): The approximation coefficients at the last level. Shape: (B, C, H/f, W/f)\n",
    "        \"\"\"\n",
    "        luminance = F.interpolate(Yl_norm, size=shape, mode='bilinear', align_corners=False)\n",
    "        to_invert = luminance < 0.5\n",
    "        luminance[to_invert] = 1 - luminance[to_invert]\n",
    "        return 1 + luminance\n",
    "\n",
    "    def heatmaps(self, x, eps=1e-6):\n",
    "        Yl, Yh = self.dwt(x)\n",
    "        shape = img_pt.shape[-2:]\n",
    "        texture = self.texture_mask(shape, Yh, Yl)\n",
    "        luminance = self.luminance_mask(shape, Yl / 2**self.level)\n",
    "        # mask = (texture+eps) ** 0.2 * luminance / 4.0\n",
    "        mask = ((texture+eps) ** 0.2) * (luminance) / 2.0\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "print(os.listdir(\"/private/home/pfz/_images\"))\n",
    "# img = \"/private/home/pfz/_images/trex_bike.png\"\n",
    "# img = \"/private/home/pfz/_images/chao.png\"\n",
    "img = \"/private/home/pfz/_images/corgi_avocado.png\"\n",
    "\n",
    "# keep only rgb channels\n",
    "img = Image.open(img, \"r\").convert(\"RGB\")\n",
    "img_pt = to_tensor(img).unsqueeze(0)\n",
    "\n",
    "jnd = DWTHeatmap()\n",
    "\n",
    "hmap = jnd.heatmaps(img_pt) \n",
    "hmap_pil = to_pil( hmap.squeeze().clamp(0, 1) ) \n",
    "\n",
    "# show img and hmap side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].imshow(img)\n",
    "axs[1].imshow(hmap_pil)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (20, 10)\n",
    "\n",
    "# choose start such that the PSNR is 50db\n",
    "target_psnr = 20\n",
    "estimated_mse = (255 / 10**(target_psnr / 20))**2 * (img_pt.shape[-1]*img_pt.shape[-2] / (img_pt.shape[-1]*img_pt.shape[-2] - 1))\n",
    "start = np.sqrt(estimated_mse)\n",
    "# choose step such that the PSNR step is 2db\n",
    "# psnr( alpha * delta ) = 10 * log10(255^2 / (alpha^2 * MSE)) = psnr( delta ) - 20 * log10(alpha)\n",
    "step_db = 5\n",
    "step = 10**(step_db/20)\n",
    "\n",
    "all_imgs_w = []\n",
    "for std in [start * step**i for i in range(10)]:\n",
    "    gaussian_noise = torch.randn_like(img_pt) * std     \n",
    "    img_w = 255 * img_pt + gaussian_noise * hmap  # b c h w\n",
    "    all_imgs_w.append(img_w)\n",
    "\n",
    "# display images with different PSNR\n",
    "fig, axs = plt.subplots(2, 5, figsize=figsize)\n",
    "for i, img_w in enumerate(all_imgs_w):\n",
    "    img_w = torch.clamp(img_w, 0, 255) / 255\n",
    "    img_w = to_pil(img_w.squeeze())\n",
    "    psnr = peak_signal_noise_ratio(np.array(img), np.array(img_w))\n",
    "    axs[i//5, i%5].imshow(img_w)\n",
    "    axs[i//5, i%5].set_title(f\"PSNR: {psnr:.2f}\")\n",
    "plt.show()\n",
    "\n",
    "# plot the diffs\n",
    "fig, axs = plt.subplots(2, 5, figsize=figsize)\n",
    "for i, img_w in enumerate(all_imgs_w):\n",
    "    img_w = torch.clamp(img_w, 0, 255) / 255\n",
    "    diff = torch.abs(img_pt - img_w)  # b c h w\n",
    "    diff = diff.squeeze()  # c h w\n",
    "    diff = to_pil(10 * diff)\n",
    "    axs[i//5, i%5].imshow(diff)\n",
    "    img_w = to_pil(img_w.squeeze())\n",
    "    psnr = peak_signal_noise_ratio(np.array(img), np.array(img_w))\n",
    "    axs[i//5, i%5].set_title(f\"PSNR: {psnr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Assuming X is your input tensor\n",
    "xfm = DWTForward(J=3, wave='haar', mode='zero')\n",
    "\n",
    "Yl, Yh = xfm(img_pt)\n",
    "# print(Yl.shape)\n",
    "# print(Yh[0].shape)\n",
    "# print(Yh[1].shape)\n",
    "# print(Yh[2].shape)\n",
    "\n",
    "# Function to normalize coefficients for visualization\n",
    "def normalize_coeffs(Yl, Yh):\n",
    "    Yl_norm = (Yl - Yl.min()) / (Yl.max() - Yl.min())\n",
    "    Yh_norm = []\n",
    "    for level in Yh:\n",
    "        level_norm = tuple((c - c.min()) / (c.max() - c.min()) for c in level)\n",
    "        Yh_norm.append(level_norm)\n",
    "    return Yl_norm, Yh_norm\n",
    "\n",
    "Yl_norm, Yh_norm = normalize_coeffs(Yl, Yh)\n",
    "\n",
    "# Function to plot the decomposition\n",
    "def plot_dwt_decomposition(Yl_norm, Yh_norm):\n",
    "    fig, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ax[i, j].axis('off')\n",
    "\n",
    "    Y_level1 = Yh_norm[0][0]  # 1 3 3 h w -> 3 3 h w\n",
    "    Y_level2 = Yh_norm[1][0]\n",
    "    Y_level3 = Yh_norm[2][0]\n",
    "\n",
    "    # Level 3\n",
    "    ax[0, 0].imshow(Yl_norm.squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[0, 0].set_title('Approximation Level 3')\n",
    "    ax[0, 1].imshow(Y_level3[:, 0].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[0, 1].set_title('Horizontal Detail Level 3')\n",
    "    ax[1, 0].imshow(Y_level3[:, 1].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[1, 0].set_title('Vertical Detail Level 3')\n",
    "    ax[1, 1].imshow(Y_level3[:, 2].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[1, 1].set_title('Diagonal Detail Level 3')\n",
    "\n",
    "    # Level 2\n",
    "    ax[1, 2].imshow(Y_level2[:, 0].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[1, 2].set_title('Horizontal Detail Level 2')\n",
    "    ax[2, 1].imshow(Y_level2[:, 1].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[2, 1].set_title('Vertical Detail Level 2')\n",
    "    ax[2, 2].imshow(Y_level2[:, 2].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[2, 2].set_title('Diagonal Detail Level 2')\n",
    "\n",
    "    # Level 1\n",
    "    ax[2, 3].imshow(Y_level1[:, 0].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[2, 3].set_title('Horizontal Detail Level 1')\n",
    "    ax[3, 2].imshow(Y_level1[:, 1].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[3, 2].set_title('Vertical Detail Level 1')\n",
    "    ax[3, 3].imshow(Y_level1[:, 2].squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "    ax[3, 3].set_title('Diagonal Detail Level 1')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_dwt_decomposition(Yl_norm, Yh_norm)\n",
    "\n",
    "# Inverse DWT\n",
    "ifm = DWTInverse(wave='haar', mode='zero')\n",
    "Y = ifm((Yl, Yh))\n",
    "\n",
    "# plot\n",
    "plt.imshow(Y.squeeze().cpu().numpy().transpose(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf = 64\n",
    "\n",
    "up_conv = nn.Sequential(\n",
    "    nn.Upsample(scale_factor = 2, mode='bilinear'),\n",
    "    nn.ReflectionPad2d(1),\n",
    "    nn.Conv2d(ngf, int(ngf / 2), kernel_size=3, stride=1, padding=0)\n",
    ")\n",
    "\n",
    "up_conv(torch.randn(1, ngf, 8, 8)).shape\n",
    "\n",
    "up_conv = nn.ConvTranspose2d(ngf, ngf//2, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "\n",
    "up_conv(torch.randn(1, ngf, 8, 8)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests pattern complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatternComplexity(nn.Module):\n",
    "    \"\"\" https://ieeexplore.ieee.org/document/7885108 \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocess=lambda x: x):\n",
    "        super(PatternComplexity, self).__init__()\n",
    "        kernel_x = torch.tensor(\n",
    "            [[-1., 0., 1.], \n",
    "            [-2., 0., 2.], \n",
    "            [-1., 0., 1.]]\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_y = torch.tensor(\n",
    "            [[1., 2., 1.], \n",
    "            [0., 0., 0.], \n",
    "            [-1., -2., -1.]]\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Expand kernels for 3 input channels and 3 output channels, apply the same filter to each channel\n",
    "        kernel_x = kernel_x.repeat(3, 1, 1, 1)\n",
    "        kernel_y = kernel_y.repeat(3, 1, 1, 1)\n",
    "\n",
    "        self.conv_x = nn.Conv2d(3, 3, kernel_size=(3, 3), padding=1, bias=False, groups=3)\n",
    "        self.conv_y = nn.Conv2d(3, 3, kernel_size=(3, 3), padding=1, bias=False, groups=3)\n",
    "\n",
    "        self.conv_x.weight = nn.Parameter(kernel_x, requires_grad=False)\n",
    "        self.conv_y.weight = nn.Parameter(kernel_y, requires_grad=False)\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def jnd_cm(self, x, beta=0.117, eps=1e-8):\n",
    "        \"\"\" Contrast masking: x must be in [0,255] \"\"\"\n",
    "        grad_x = self.conv_x(x)\n",
    "        grad_y = self.conv_y(x)\n",
    "        cm = torch.sqrt(grad_x**2 + grad_y**2)\n",
    "        return beta * cm\n",
    "    \n",
    "    def theta(self, x):\n",
    "        \"\"\" x must be in [0,255] \"\"\"\n",
    "        grad_x = self.conv_x(x)\n",
    "        grad_y = self.conv_y(x)\n",
    "        return torch.atan2(grad_y, grad_x)\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def heatmaps(\n",
    "        self, \n",
    "        imgs: torch.Tensor, \n",
    "        clc: float = 0.3, \n",
    "        input_method = \"multi_channels\",\n",
    "        output_method = \"multi_channels\"\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\" imgs must be in [0,1] after preprocess \"\"\"\n",
    "        imgs = self.preprocess(imgs)\n",
    "        # imgs = 255 * self.preprocess(imgs)\n",
    "        # rgbs = torch.tensor([0.299, 0.587, 0.114])\n",
    "        if input_method == 'single_channels':\n",
    "            # imgs = imgs[...,0:1,:,:] + imgs[...,1:2,:,:] + imgs[...,2:3,:,:]\n",
    "            imgs = imgs[...,0:1,:,:] + imgs[...,1:2,:,:] + imgs[...,2:3,:,:]\n",
    "            imgs = imgs.repeat(1, 3, 1, 1)  # hack to make it work with the multi_channels method\n",
    "        hmaps = self.theta(imgs)\n",
    "        # hmaps = self.jnd_cm(imgs)\n",
    "        if output_method == \"multi_channels\":\n",
    "            # rgbs = (1-rgbs).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            # return  hmaps * rgbs.to(hmaps.device)  # b 3 h w\n",
    "            return  hmaps\n",
    "        elif output_method == \"single_channels\":\n",
    "            # rgbs = (1-rgbs).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            return torch.sum(\n",
    "                hmaps, \n",
    "                # hmaps * rgbs.to(hmaps.device), \n",
    "                dim=1, keepdim=True\n",
    "            )  # b c h w * 1 c -> b 1 h w\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor, deltas: torch.Tensor, alpha: float = 1.0, input_method = \"multi_channels\", output_method = \"multi_channels\") -> torch.Tensor:\n",
    "        \"\"\" imgs and deltas must be in [0,1] after preprocess \"\"\"\n",
    "        hmaps = self.heatmaps(imgs, clc=0.3, input_method=input_method, output_method=output_method)\n",
    "        return imgs + alpha * hmaps * deltas\n",
    "\n",
    "\n",
    "class PCLoss(nn.Module):\n",
    "    def __init__(self, \n",
    "        preprocess = lambda x: x,\n",
    "        loss_type: int = 0\n",
    "    ):\n",
    "        super(PCLoss, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.pc = PatternComplexity(preprocess=preprocess)\n",
    "        # self.msa = lambda x, y: 1 - torch.cos(x - y)\n",
    "        # self.mse = nn.MSELoss()\n",
    "        angular_diff = lambda x, y: torch.min((x - y) ** 2, (360 - torch.abs(x - y)) ** 2)\n",
    "        self.msa = lambda x, y: angular_diff(x, y).mean()\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        imgs: torch.Tensor,\n",
    "        imgs_w: torch.Tensor,\n",
    "    ):\n",
    "        # thetas_o = self.pc.heatmaps(imgs, input_method=\"single_channels\", output_method=\"single_channels\")\n",
    "        # thetas_delta = self.pc.heatmaps(deltas, input_method=\"single_channels\", output_method=\"single_channels\")\n",
    "        thetas_o = self.pc.heatmaps(imgs, input_method=\"single_channels\", output_method=\"single_channels\")  # b 1 h w\n",
    "        deltas = imgs_w - imgs  \n",
    "        thetas_delta = self.pc.heatmaps(deltas, input_method=\"single_channels\", output_method=\"single_channels\")  # b 1 h w\n",
    "        if self.loss_type == 0:\n",
    "            loss = self.msa(thetas_o, thetas_delta).mean(dim=(1,2,3))\n",
    "        elif self.loss_type == 1:\n",
    "            loss = (self.msa(thetas_o, thetas_delta) * torch.mean(deltas**2, dim=1, keepdim=True)).mean(dim=(1,2,3))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid loss_type: {self.loss_type}\")\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"Loss has nans\")\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "print(os.listdir(\"/private/home/pfz/_images\"))\n",
    "img = \"/private/home/pfz/_images/trex_bike.png\"\n",
    "img = \"/private/home/pfz/_images/chao.png\"\n",
    "img = \"/private/home/pfz/_images/corgi_avocado.png\"\n",
    "img = Image.open(img, \"r\").convert(\"RGB\")\n",
    "img_pt = to_tensor(img).unsqueeze(0)\n",
    "\n",
    "pc = PatternComplexity()\n",
    "\n",
    "INPUT_METHODS = [\"single_channels\", \"multi_channels\"]\n",
    "OUTPUT_METHODS = [\"single_channels\", \"multi_channels\"]\n",
    "for OUTPUT_METHOD in OUTPUT_METHODS:\n",
    "    for INPUT_METHOD in INPUT_METHODS:\n",
    "        print(INPUT_METHOD, OUTPUT_METHOD)\n",
    "        hmap = pc.heatmaps(img_pt, input_method=INPUT_METHOD, output_method=OUTPUT_METHOD)\n",
    "        hmap = (hmap - hmap.min()) / (hmap.max() - hmap.min())\n",
    "        hmap_pil = to_pil(hmap.squeeze())\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axs[0].imshow(img)\n",
    "        if OUTPUT_METHOD == \"multi_channels\":\n",
    "            axs[1].imshow(hmap_pil)\n",
    "        else:\n",
    "            hmap_pil = to_pil(hmap.squeeze())\n",
    "            axs[1].imshow(hmap_pil, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "# hmap = pc.heatmaps(img_pt, input_method=INPUT_METHOD, output_method=OUTPUT_METHOD)\n",
    "# print(hmap.max(), hmap.min())\n",
    "# hmap = (hmap - hmap.min()) / (hmap.max() - hmap.min())\n",
    "# hmap_pil = to_pil(hmap.squeeze())\n",
    "\n",
    "# # show img and hmap side by side\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# axs[0].imshow(img)\n",
    "# axs[1].imshow(hmap_pil)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests JND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JND(nn.Module):\n",
    "    \"\"\" https://ieeexplore.ieee.org/document/7885108 \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocess=lambda x: x):\n",
    "        super(JND, self).__init__()\n",
    "        kernel_x = torch.tensor(\n",
    "            [[-1., 0., 1.], \n",
    "            [-2., 0., 2.], \n",
    "            [-1., 0., 1.]]\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_y = torch.tensor(\n",
    "            [[1., 2., 1.], \n",
    "            [0., 0., 0.], \n",
    "            [-1., -2., -1.]]\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_lum = torch.tensor(\n",
    "            [[1., 1., 1., 1., 1.], \n",
    "             [1., 2., 2., 2., 1.], \n",
    "             [1., 2., 0., 2., 1.], \n",
    "             [1., 2., 2., 2., 1.], \n",
    "             [1., 1., 1., 1., 1.]]\n",
    "        ).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Expand kernels for 3 input channels and 3 output channels, apply the same filter to each channel\n",
    "        kernel_x = kernel_x.repeat(3, 1, 1, 1)\n",
    "        kernel_y = kernel_y.repeat(3, 1, 1, 1)\n",
    "        kernel_lum = kernel_lum.repeat(3, 1, 1, 1)\n",
    "\n",
    "        self.conv_x = nn.Conv2d(3, 3, kernel_size=(3, 3), padding=1, bias=False, groups=3)\n",
    "        self.conv_y = nn.Conv2d(3, 3, kernel_size=(3, 3), padding=1, bias=False, groups=3)\n",
    "        self.conv_lum = nn.Conv2d(3, 3, kernel_size=(5, 5), padding=2, bias=False, groups=3)\n",
    "\n",
    "        self.conv_x.weight = nn.Parameter(kernel_x, requires_grad=False)\n",
    "        self.conv_y.weight = nn.Parameter(kernel_y, requires_grad=False)\n",
    "        self.conv_lum.weight = nn.Parameter(kernel_lum, requires_grad=False)\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "    \n",
    "    def jnd_la(self, x, alpha=1.0, eps=1e-5):\n",
    "        \"\"\" Luminance masking: x must be in [0,255] \"\"\"\n",
    "        la = self.conv_lum(x) / 32\n",
    "        mask_lum = la <= 127\n",
    "        la[mask_lum] = 17 * (1 - torch.sqrt(la[mask_lum]/127 + eps))\n",
    "        la[~mask_lum] = 3/128 * (la[~mask_lum] - 127) + 3\n",
    "        return alpha * la\n",
    "\n",
    "    def jnd_cm(self, x, beta=0.117, eps=1e-5):\n",
    "        \"\"\" Contrast masking: x must be in [0,255] \"\"\"\n",
    "        grad_x = self.conv_x(x)\n",
    "        grad_y = self.conv_y(x)\n",
    "        cm = torch.sqrt(grad_x**2 + grad_y**2)\n",
    "        cm = 16 * cm**2.4 / (cm**2 + 26**2)\n",
    "        return beta * cm\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    def heatmaps(\n",
    "        self, \n",
    "        imgs: torch.Tensor, \n",
    "        clc: float = 0.3, \n",
    "        input_method = \"multi_channels\",\n",
    "        output_method = \"multi_channels\"\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\" imgs must be in [0,1] after preprocess \"\"\"\n",
    "        imgs = 255 * self.preprocess(imgs)\n",
    "        rgbs = torch.tensor([0.299, 0.587, 0.114])\n",
    "        if input_method == 'single_channels':\n",
    "            imgs = rgbs[0] * imgs[...,0:1,:,:] + rgbs[1] * imgs[...,1:2,:,:] + rgbs[2] * imgs[...,2:3,:,:]\n",
    "            imgs = imgs.repeat(1, 3, 1, 1)  # hack to make it work with the multi_channels method\n",
    "        la = self.jnd_la(imgs)\n",
    "        cm = self.jnd_cm(imgs)\n",
    "        hmaps = torch.clamp_min(la + cm - clc * torch.minimum(la, cm), 0) / 255\n",
    "        if output_method == \"multi_channels\":\n",
    "            rgbs = (1-rgbs).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            return  hmaps * rgbs.to(hmaps.device)  # b 3 h w\n",
    "        elif output_method == \"single_channels\":\n",
    "            rgbs = (1-rgbs).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            return torch.sum(\n",
    "                hmaps * rgbs.to(hmaps.device), \n",
    "                dim=1, keepdim=True\n",
    "            )  # b c h w * 1 c -> b 1 h w\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor, deltas: torch.Tensor, alpha: float = 1.0, beta: float = 0.117, input_method = \"multi_channels\", output_method = \"multi_channels\") -> torch.Tensor:\n",
    "        \"\"\" imgs and deltas must be in [0,1] after preprocess \"\"\"\n",
    "        hmaps = self.heatmaps(imgs, clc=0.3, input_method=input_method, output_method=output_method)\n",
    "        return imgs + alpha * hmaps * deltas\n",
    "\n",
    "\n",
    "\n",
    "class JNDLoss(nn.Module):\n",
    "    def __init__(self, \n",
    "        preprocess = lambda x: x,\n",
    "        loss_type = 0\n",
    "    ):\n",
    "        super(JNDLoss, self).__init__()\n",
    "        self.jnd = JND(preprocess)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.loss_type = loss_type\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        imgs: torch.Tensor,\n",
    "        imgs_w: torch.Tensor,\n",
    "    ):\n",
    "        jnds = self.jnd.heatmaps(imgs)  # b 1 h w\n",
    "        deltas = imgs_w - imgs  # b c h w\n",
    "        if self.loss_type == 0:\n",
    "            loss = self.mse(1.0 * deltas.abs(), jnds)\n",
    "            return loss\n",
    "        elif self.loss_type == 1:\n",
    "            loss = self.mse(1.0 * deltas * jnds, torch.zeros_like(deltas))\n",
    "            return loss\n",
    "        else:\n",
    "            raise ValueError(f\"Loss type {self.loss_type} not supported. Use 0 or 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "print(os.listdir(\"/private/home/pfz/_images\"))\n",
    "img = \"/private/home/pfz/_images/trex_bike.png\"\n",
    "# img = \"/private/home/pfz/_images/chao.png\"\n",
    "# img = \"/private/home/pfz/_images/corgi_avocado.png\"\n",
    "\n",
    "# keep only rgb channels\n",
    "img = Image.open(img, \"r\").convert(\"RGB\")\n",
    "img_pt = to_tensor(img).unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnd = JND()\n",
    "jnd_loss = JNDLoss()\n",
    "\n",
    "hmap = jnd.heatmaps(img_pt)\n",
    "print(hmap.max(), hmap.min())\n",
    "hmap_pil = to_pil(10 * hmap.squeeze())\n",
    "\n",
    "# show img and hmap side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].imshow(img)\n",
    "axs[1].imshow(hmap_pil)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (24, 10)\n",
    "\n",
    "# choose start such that the PSNR is 50db\n",
    "target_psnr = 40\n",
    "estimated_mse = (255 / 10**(target_psnr / 20))**2 * (img_pt.shape[-1]*img_pt.shape[-2] / (img_pt.shape[-1]*img_pt.shape[-2] - 1))\n",
    "start = np.sqrt(estimated_mse)\n",
    "# choose step such that the PSNR step is 2db\n",
    "# psnr( alpha * delta ) = 10 * log10(255^2 / (alpha^2 * MSE)) = psnr( delta ) - 20 * log10(alpha)\n",
    "step_db = 3\n",
    "step = 10**(step_db/20)\n",
    "\n",
    "all_imgs_w = []\n",
    "all_psnrs = []\n",
    "for std in [start * step**i for i in range(10)]:\n",
    "    gaussian_noise = torch.randn_like(img_pt) * std\n",
    "    psnr = 10 * torch.log10(255**2 / (torch.mean(gaussian_noise**2)))\n",
    "    all_psnrs.append(psnr)\n",
    "    img_w = 255 * img_pt + gaussian_noise\n",
    "    all_imgs_w.append(img_w)\n",
    "\n",
    "# display images with different PSNR\n",
    "fig, axs = plt.subplots(2, 5, figsize=figsize)\n",
    "for i, (img_w, psnr) in enumerate(zip(all_imgs_w, all_psnrs)):\n",
    "    img_w = torch.clamp(img_w, 0, 255) / 255\n",
    "    img_w = img_w.squeeze()\n",
    "    img_w = to_pil(img_w)\n",
    "    axs[i//5, i%5].imshow(img_w)\n",
    "    axs[i//5, i%5].set_title(f\"PSNR: {psnr:.2f}\")\n",
    "    print(peak_signal_noise_ratio(np.array(img), np.array(img_w)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose start such that the PSNR is 50db\n",
    "target_psnr = 20\n",
    "estimated_mse = (255 / 10**(target_psnr / 20))**2 * (img_pt.shape[-1]*img_pt.shape[-2] / (img_pt.shape[-1]*img_pt.shape[-2] - 1))\n",
    "start = np.sqrt(estimated_mse)\n",
    "# choose step such that the PSNR step is 2db\n",
    "# psnr( alpha * delta ) = 10 * log10(255^2 / (alpha^2 * MSE)) = psnr( delta ) - 20 * log10(alpha)\n",
    "step_db = 5\n",
    "step = 10**(step_db/20)\n",
    "\n",
    "all_imgs_w = []\n",
    "for std in [start * step**i for i in range(10)]:\n",
    "    gaussian_noise = torch.randn_like(img_pt) * std     \n",
    "    img_w = 255 * img_pt + gaussian_noise * hmap  # b c h w\n",
    "    all_imgs_w.append(img_w)\n",
    "\n",
    "# display images with different PSNR\n",
    "fig, axs = plt.subplots(2, 5, figsize=figsize)\n",
    "for i, img_w in enumerate(all_imgs_w):\n",
    "    img_w = torch.clamp(img_w, 0, 255) / 255\n",
    "    img_w = to_pil(img_w.squeeze())\n",
    "    psnr = peak_signal_noise_ratio(np.array(img), np.array(img_w))\n",
    "    axs[i//5, i%5].imshow(img_w)\n",
    "    axs[i//5, i%5].set_title(f\"PSNR: {psnr:.2f}\")\n",
    "plt.show()\n",
    "\n",
    "# plot the diffs\n",
    "fig, axs = plt.subplots(2, 5, figsize=figsize)\n",
    "for i, img_w in enumerate(all_imgs_w):\n",
    "    img_w = torch.clamp(img_w, 0, 255) / 255\n",
    "    diff = torch.abs(img_pt - img_w)  # b c h w\n",
    "    diff = diff.squeeze()  # c h w\n",
    "    diff = to_pil(10 * diff)\n",
    "    axs[i//5, i%5].imshow(diff)\n",
    "    img_w = to_pil(img_w.squeeze())\n",
    "    psnr = peak_signal_noise_ratio(np.array(img), np.array(img_w))\n",
    "    axs[i//5, i%5].set_title(f\"PSNR: {psnr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "[model for model in timm.list_models() if 'dino' in model.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://dl.fbaipublicfiles.com/dinov2/images/example.jpg'\n",
    "))\n",
    "\n",
    "model = timm.create_model(\n",
    "    'vit_small_patch14_reg4_dinov2',\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # remove classifier nn.Linear\n",
    ")\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n",
    "\n",
    "# or equivalently (without needing to set num_classes=0)\n",
    "\n",
    "output = model.forward_features(transforms(img).unsqueeze(0))\n",
    "print(output.shape)  # torch.Size([1, 197, 384]\n",
    "# output is unpooled, a (1, 197, 384) shaped tensor\n",
    "\n",
    "# output = model.forward_head(output, pre_logits=True)\n",
    "# # output is a (1, num_features) shaped tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "img = default_transform(img).unsqueeze(0)\n",
    "\n",
    "output = model(img)\n",
    "print(output.shape)  # torch.Size([1, 197, 384])\n",
    "\n",
    "output = model.forward_features(img)\n",
    "print(output.shape)  # torch.Size([1, 197, 384])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('samvit_base_patch16.sa1b', pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "img = default_transform(img).unsqueeze(0)\n",
    "\n",
    "model(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# DINOv2\n",
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dinov2_vitb14.to('cuda')\n",
    "img_size = 224\n",
    "\n",
    "# default_transform = transforms.Compose([\n",
    "#     transforms.Resize(img_size),\n",
    "#     transforms.CenterCrop(img_size),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to('cuda')\n",
    "model(dummy_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/dinov2/blob/main/notebooks/semantic_segmentation.ipynb\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "HEAD_DATASET = \"voc2012\" # in (\"ade20k\", \"voc2012\")\n",
    "HEAD_TYPE = \"ms\" # in (\"ms, \"linear\")\n",
    "backbone_arch = \"vits14\"\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "print(f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\")\n",
    "\n",
    "!wget {head_config_url} -O head_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation in dinov2 uses a head on top of the backbone that takes indices [8, 9, 10, 11] of the intermediate layers\n",
    "latents = model.get_intermediate_layers(dummy_img, reshape=True, n=[8, 9, 10, 11])\n",
    "for i, latent in enumerate(latents):\n",
    "    print(f'Layer {i}: {latent.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(latents, dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 2\n",
    "nbits = 3\n",
    "h, w = 4, 4\n",
    "\n",
    "preds = torch.rand(bsz, 1+nbits, h, w)  # b 1+nbits h w\n",
    "msgs = torch.randint(0, 2, (bsz, nbits))  # b nbits\n",
    "masks = torch.randint(0, 2, (bsz, 1, h, w))  # b 1 h w\n",
    "\n",
    "msg_preds = preds[:, 1:, :, :]  # b nbits h w\n",
    "msg_targets = msgs.unsqueeze(-1).unsqueeze(-1).expand_as(msg_preds)  # b nbits h w\n",
    "# flatten and select pixels where mask is =1\n",
    "msg_preds = msg_preds.masked_select(\n",
    "    masks.expand_as(msg_preds)  # b 1 h w -> b nbits h w\n",
    ").view(-1, msg_preds.size(1))  # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 3\n",
    "nbits = 16\n",
    "hidden_size = 512\n",
    "\n",
    "embedding = torch.nn.Embedding(nbits, hidden_size)\n",
    "msg = torch.randn((bsz, hidden_size))\n",
    "indices = torch.arange(0, nbits-1).long()\n",
    "msg = embedding(indices)\n",
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data - COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m chameleon.download_data \"https://dbj4kdlgwwbrp.cloudfront.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidmU0MjV0a3lhd3JsbzdrZDZicnVybTJnIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGJqNGtkbGd3d2JycC5jbG91ZGZyb250Lm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODk2MjI3NH19fV19&Signature=VBWPADG5ku3AuB7tGVTnXthTAaKyrWacczU4fAwZcbRw-UslWVicp69oWUccsLC-0c0WWZgRzCi5ZLwJQMGxD3z7Xf9FoCkmHAzlNSPIQ%7E24kH8Bvu77aOC3LTxrmp%7ECxtVsbwKBKbQz3iPAuiF6d8RV2Li9p%7ENUc3EMwldfxwIQXrvdpC6yY0CoRq%7EMay-i4bD7d3RZg-vom5l9gBeE3AuXAE2fhv0aJfy0OEAgv41BVFVQXePseYCiMDUiAu6IbFxBpx8eurXyqdptDR%7EIxUhshYwomYa2C81AQ6Tnwio6lP8s1UVGeNdpg4TEKp7GtDElBqT50zQIzTZQmXyItA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=828925129156152\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "\n",
    "# Define the data directory\n",
    "# imgs_dir = \"/datasets01/COCO-Stuff/042623/face_blurred/train_img\"\n",
    "imgs_dir = \"/datasets01/COCO/060817/train2014/\"\n",
    "anns_dir = \"/datasets01/COCO/060817/annotations/instances_train2014.json\"\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    # v2.ToImage(),\n",
    "    v2.RandomPhotometricDistort(p=1),\n",
    "    v2.RandomCrop(256),\n",
    "    v2.RandomHorizontalFlip(p=1),\n",
    "])\n",
    "\n",
    "# Load the COCO dataset\n",
    "coco = CocoDetection(root = imgs_dir,\n",
    "                     annFile = anns_dir,\n",
    "                     transform = train_transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show pil image\n",
    "img = coco[0][0]\n",
    "transformed_img, transformed_mask = train_transforms(img, mask)\n",
    "transformed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transformed_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "img, anns = coco.__getitem__(0)\n",
    "\n",
    "mask = coco.coco.annToMask(anns[0])\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display the masks\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "imgs_dir = \"/datasets01/COCO-Stuff/042623/face_blurred/train_img\"\n",
    "anns_dir = \"/datasets01/COCO/060817/annotations/instances_train2014.json\"\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the COCO dataset\n",
    "coco = CocoDetection(root = imgs_dir,\n",
    "                     annFile = anns_dir,\n",
    "                     transform = transform)\n",
    "\n",
    "for key in coco.coco.imgs:\n",
    "    dict_key = coco.coco.imgs[key]\n",
    "    dict_key['file_name'] = dict_key['file_name'].replace('COCO_train2014_', '')\n",
    "\n",
    "def get_segmentation_mask(image_id):\n",
    "    annIds = coco.coco.getAnnIds(imgIds=image_id, iscrowd=None)\n",
    "    anns = coco.coco.loadAnns(annIds)\n",
    "    masks = []\n",
    "    for ann in anns:\n",
    "        mask = coco.coco.annToMask(ann)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = np.stack(masks, axis=0)\n",
    "    else:\n",
    "        masks = np.zeros((0, image_id['height'], image_id['width']), dtype=np.uint8)\n",
    "    return masks\n",
    "\n",
    "def get_item(index):\n",
    "    img, target = coco[index]\n",
    "    image_id = coco.ids[index]\n",
    "    masks = get_segmentation_mask(image_id)\n",
    "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    return img, masks\n",
    "\n",
    "# Test the function\n",
    "img, masks = get_item(0)\n",
    "print(img.shape)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test perceptual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.losses.watson_fft import ColorWrapper, WatsonDistanceFft\n",
    "from videoseal.data.transforms import normalize_img\n",
    "\n",
    "perceptual_loss = ColorWrapper(WatsonDistanceFft, (), {\"reduction\": \"none\"})\n",
    "ckpt_loss = \"/checkpoint/pfz/projects/ai_signature/loss_weights/rgb_watson_fft_trial0.pth\"\n",
    "perceptual_loss.load_state_dict(torch.load(ckpt_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptual_loss(\n",
    "    img.unsqueeze(0),\n",
    "    0.001 + img.unsqueeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import videoseal.modeling\n",
    "importlib.reload(videoseal.modeling)\n",
    "\n",
    "image_size = 256\n",
    "vit_patch_size = 16\n",
    "\n",
    "model = 'tiny'\n",
    "if model == 'base':\n",
    "    encoder_embed_dim = 768\n",
    "    encoder_depth = 12\n",
    "    encoder_num_heads = 12\n",
    "elif model == 'small':\n",
    "    encoder_embed_dim = 384\n",
    "    encoder_depth = 12\n",
    "    encoder_num_heads = 6\n",
    "elif model == 'tiny':\n",
    "    encoder_embed_dim = 192\n",
    "    encoder_depth = 12\n",
    "    encoder_num_heads = 3\n",
    "\n",
    "encoder_global_attn_indexes = [2, 5, 8, 11]\n",
    "prompt_embed_dim = 512\n",
    "\n",
    "image_embedding_size = image_size // vit_patch_size\n",
    "image_encoder=videoseal.modeling.ImageEncoderViT(\n",
    "    depth=encoder_depth,\n",
    "    embed_dim=encoder_embed_dim,\n",
    "    img_size=image_size,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=encoder_num_heads,\n",
    "    patch_size=vit_patch_size,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=encoder_global_attn_indexes,\n",
    "    window_size=14,\n",
    "    # window_size=14,\n",
    "    out_chans=prompt_embed_dim,\n",
    ")\n",
    "pixel_decoder = videoseal.modeling.PixelDecoder(\n",
    "    embed_dim=prompt_embed_dim\n",
    ")\n",
    "\n",
    "wam = videoseal.modeling.Wam(\n",
    "    image_encoder=image_encoder,\n",
    "    pixel_decoder=pixel_decoder,\n",
    "    pixel_mean=[123.675, 116.28, 103.53],\n",
    "    pixel_std=[58.395, 57.12, 57.375],\n",
    ")\n",
    "\n",
    "print('total parameters img_encoder: %d'%sum(p.numel() for p in wam.image_encoder.parameters()))\n",
    "print('total parameters pixel_decoder: %d'%sum(p.numel() for p in wam.pixel_decoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = img.unsqueeze(0)\n",
    "imgs = wam.preprocess(imgs)\n",
    "image_embeddings = wam.image_encoder(imgs)\n",
    "print(imgs.shape)\n",
    "print(image_embeddings.shape)\n",
    "\n",
    "decoded = wam.pixel_decoder(image_embeddings)\n",
    "print(decoded.shape)\n",
    "\n",
    "imshow(imgs[0], pixel_mean=wam.pixel_mean, pixel_std=wam.pixel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation massk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "imgs_dir = \"/datasets01/COCO-Stuff/042623/face_blurred/train_img\"\n",
    "anns_dir = \"/datasets01/COCO/060817/annotations/instances_train2014.json\"\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the COCO dataset\n",
    "coco = CocoDetection(root = imgs_dir,\n",
    "                     annFile = anns_dir,\n",
    "                     transform = transform)\n",
    "\n",
    "for key in coco.coco.imgs:\n",
    "    dict_key = coco.coco.imgs[key]\n",
    "    dict_key['file_name'] = dict_key['file_name'].replace('COCO_train2014_', '')\n",
    "\n",
    "def get_segmentation_mask(image_id):\n",
    "    annIds = coco.coco.getAnnIds(imgIds=image_id, iscrowd=None)\n",
    "    anns = coco.coco.loadAnns(annIds)\n",
    "    masks = []\n",
    "    for ann in anns:\n",
    "        mask = coco.coco.annToMask(ann)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = np.stack(masks, axis=0)\n",
    "    else:\n",
    "        masks = np.zeros((0, image_id['height'], image_id['width']), dtype=np.uint8)\n",
    "    return masks\n",
    "\n",
    "def get_item(index):\n",
    "    img, target = coco[index]\n",
    "    image_id = coco.ids[index]\n",
    "    masks = get_segmentation_mask(image_id)\n",
    "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "    return img, masks\n",
    "\n",
    "# Test the function\n",
    "img, masks = get_item(0)\n",
    "print(img.shape)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images and segmentation masks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_image_and_masks(img, masks):\n",
    "    # Convert the image tensor to a PIL Image\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    \n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # sequential colors\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, 10))\n",
    "    \n",
    "    # Plot each segmentation mask\n",
    "    for ii, mask in enumerate(masks):\n",
    "        # Create a custom colormap for the segmentation masks\n",
    "        mask_color = [(0,0,0,0), colors[ii]]  # first color is transparent, second is blue\n",
    "        cmap = mcolors.LinearSegmentedColormap.from_list('Custom', mask_color, N=2)\n",
    "    \n",
    "        plt.imshow(mask, cmap=cmap, alpha=0.5)  # alpha is for transparency\n",
    "    \n",
    "    # plot legend for each mask\n",
    "    patches = [plt.plot([],[], marker=\"s\", ms=10, ls=\"\", mec=None, color=colors[i], \n",
    "                        label=\"Mask {}\".format(i))[0]  for i in range(len(masks))]\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test the function\n",
    "for ii in range(5):\n",
    "    img, masks = get_item(ii)\n",
    "    plot_image_and_masks(img, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
