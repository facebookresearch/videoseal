{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/pfz/miniconda3/envs/img/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/private/home/pfz/miniconda3/envs/img/lib/python3.12/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from compressai.zoo import models\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bmshj2018-factorized': <function compressai.zoo.image.bmshj2018_factorized(quality, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'bmshj2018-factorized-relu': <function compressai.zoo.image.bmshj2018_factorized_relu(quality, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'bmshj2018-hyperprior': <function compressai.zoo.image.bmshj2018_hyperprior(quality, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'mbt2018-mean': <function compressai.zoo.image.mbt2018_mean(quality, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'mbt2018': <function compressai.zoo.image.mbt2018(quality, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'cheng2020-anchor': <function compressai.zoo.image.cheng2020_anchor(quality, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'cheng2020-attn': <function compressai.zoo.image.cheng2020_attn(quality, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'bmshj2018-hyperprior-vbr': <function compressai.zoo.image_vbr.bmshj2018_hyperprior_vbr(quality=0, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'mbt2018-mean-vbr': <function compressai.zoo.image_vbr.mbt2018_mean_vbr(quality=0, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'mbt2018-vbr': <function compressai.zoo.image_vbr.mbt2018_vbr(quality=0, metric='mse', pretrained=False, progress=True, **kwargs)>,\n",
       " 'hrtzxf2022-pcc-rec': None,\n",
       " 'sfu2023-pcc-rec-pointnet': None,\n",
       " 'sfu2024-pcc-rec-pointnet2-ssg': None,\n",
       " 'ssf2020': <function compressai.zoo.video.ssf2020(quality, metric='mse', pretrained=False, progress=True, **kwargs)>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mbt2018-mean model with quality 1\n",
      "Model architecture: MeanScaleHyperprior\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model (options include: 'bmshj2018-factorized', 'mbt2018-mean', \n",
    "# 'cheng2020-anchor', 'mbt2018', etc.)\n",
    "model_name = \"mbt2018-mean\"\n",
    "quality = 1  # Quality level (1-8, higher means better quality but lower compression)\n",
    "\n",
    "# Load the model\n",
    "model = models[model_name](quality=quality, pretrained=True)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Loaded {model_name} model with quality {quality}\")\n",
    "print(f\"Model architecture: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input images must have the same dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m img_r \u001b[38;5;241m=\u001b[39m img_r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_hat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m img_r \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToPILImage()(img_r\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m psnr \u001b[38;5;241m=\u001b[39m \u001b[43mpeak_signal_noise_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_r\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img_r)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     11\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(diff)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m~/miniconda3/envs/img/lib/python3.12/site-packages/skimage/metrics/simple_metrics.py:143\u001b[0m, in \u001b[0;36mpeak_signal_noise_ratio\u001b[0;34m(image_true, image_test, data_range)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpeak_signal_noise_ratio\u001b[39m(image_true, image_test, \u001b[38;5;241m*\u001b[39m, data_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    Compute the peak signal to noise ratio (PSNR) for an image.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[43mcheck_shape_equality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_range \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m image_true\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m image_test\u001b[38;5;241m.\u001b[39mdtype:\n",
      "File \u001b[0;32m~/miniconda3/envs/img/lib/python3.12/site-packages/skimage/_shared/utils.py:620\u001b[0m, in \u001b[0;36mcheck_shape_equality\u001b[0;34m(*images)\u001b[0m\n\u001b[1;32m    618\u001b[0m image0 \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(image0\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m image\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput images must have the same dimensions.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Input images must have the same dimensions."
     ]
    }
   ],
   "source": [
    "img = \"/private/home/pfz/_images/trex_bike.png\"\n",
    "img = Image.open(img, \"r\").convert(\"RGB\")\n",
    "img = img.resize((250, 250), Image.LANCZOS)\n",
    "img_r = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "img_r = model(img_r)\n",
    "img_r = img_r['x_hat'].clamp_(0, 1)\n",
    "img_r = transforms.ToPILImage()(img_r.squeeze(0))\n",
    "psnr = peak_signal_noise_ratio(np.array(img), np.array(img_r))\n",
    "\n",
    "diff = np.array(img).astype(np.float32) - np.array(img_r).astype(np.float32)\n",
    "diff = np.abs(diff).astype(np.uint8)\n",
    "# Y channel\n",
    "diff = 0.299 * diff[:, :, 0] + 0.587 * diff[:, :, 1] + 0.114 * diff[:, :, 2]\n",
    "diff = Image.fromarray( 10 * diff )\n",
    "\n",
    "# shwo image, image recons, and difference\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[1].imshow(img_r)\n",
    "ax[1].set_title(\"Reconstructed\")\n",
    "ax[2].imshow(diff)\n",
    "ax[2].set_title(f\"Difference - PSNR: {psnr:.2f}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient exists. Gradient norm: 0.000067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set requires_grad to track gradients\n",
    "x = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "x.requires_grad = True\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.set_grad_enabled(True):\n",
    "    # Make sure the model is in train mode for gradient tracking\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(x)\n",
    "    x_hat = out['x_hat']\n",
    "    \n",
    "    # Define a loss function (MSE between reconstructed image and a target)\n",
    "    # Here we'll use the original image as the target for simplicity\n",
    "    loss = torch.nn.functional.mse_loss(x_hat, x)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check if gradient exists\n",
    "    if x.grad is not None:\n",
    "        print(f\"Gradient exists. Gradient norm: {x.grad.norm().item():.6f}\")\n",
    "    else:\n",
    "        print(\"No gradient was computed for the input.\")\n",
    "    \n",
    "    # Reset model to eval mode\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
