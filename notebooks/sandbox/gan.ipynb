{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128 * 7 * 7),  # bsz d -> bsz 128*7*7\n",
    "            nn.ReLU(True), \n",
    "            nn.Unflatten(1, (128, 7, 7)),  # bsz 128*7*7 -> bsz 128 7 7\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),  # bsz 128 7 7 -> bsz 64 14 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),  # bsz 64 14 14 -> bsz 32 28 28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),  # bsz 32 28 28 -> bsz 1 28 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1, bias=False),  # bsz 1 28 28 -> bsz 32 14 14\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False),  # bsz 32 14 14 -> bsz 64 7 7\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Flatten(),  # bsz 64 7 7 -> bsz 64*7*7\n",
    "            nn.Linear(64*7*7, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Loss function\n",
    "def gan_loss(real_logits, fake_logits, optimizer_idx):\n",
    "    if optimizer_idx == 0:  # generator update\n",
    "        return -torch.mean(fake_logits)\n",
    "    elif optimizer_idx == 1:  # discriminator update\n",
    "        real_loss = torch.mean(torch.nn.ReLU()(1.0 - real_logits))\n",
    "        fake_loss = torch.mean(torch.nn.ReLU()(1.0 + fake_logits))\n",
    "        return real_loss + fake_loss\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def freeze_grads(model):\n",
    "    \"\"\"\n",
    "    Temporarily freezes the parameters of a PyTorch model.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model whose parameters will be frozen.\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    original_requires_grad = {}\n",
    "    for param in model.parameters():\n",
    "        original_requires_grad[param] = param.requires_grad\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for param, requires_grad in original_requires_grad.items():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "# Training loop\n",
    "def train_gan(generator, discriminator, data_loader, noise_dim, num_epochs, lr, version, indices):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizers = [optimizer_G, optimizer_D]\n",
    "\n",
    "    all_stats = []\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        generator.train()\n",
    "\n",
    "        stats_per_epoch = []\n",
    "        for real_images, _ in data_loader:\n",
    "            real_images = real_images.view(real_images.size(0), 1, 28, 28).to(device)\n",
    "            batch_size = real_images.size(0)\n",
    "\n",
    "            # Pass forward generator\n",
    "            noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "            fake_images = generator(noise)\n",
    "\n",
    "            # Train 2 steps\n",
    "            stat = {}\n",
    "\n",
    "            if version == \"v0\":\n",
    "                for optimizer_idx in indices:\n",
    "                    if optimizer_idx == 0:  # generator update\n",
    "                        fake_logits = discriminator(fake_images)\n",
    "                        loss = gan_loss(None, fake_logits, optimizer_idx=0)\n",
    "                    elif optimizer_idx == 1:  # discriminator update\n",
    "                        real_logits = discriminator(real_images)\n",
    "                        fake_logits = discriminator(fake_images.detach())\n",
    "                        loss = gan_loss(real_logits, fake_logits, optimizer_idx=1)\n",
    "                    optimizers[optimizer_idx].zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizers[optimizer_idx].step()\n",
    "                    stat[f'loss_{optimizer_idx}'] = loss.item()\n",
    " \n",
    "            elif version == \"v1\":\n",
    "                for optimizer_idx in indices:\n",
    "                    optimizers[optimizer_idx].zero_grad()\n",
    "                for optimizer_idx in indices:\n",
    "                    if optimizer_idx == 0:  # generator update\n",
    "                        with freeze_grads(discriminator):\n",
    "                            fake_logits = discriminator(fake_images)\n",
    "                            loss = gan_loss(None, fake_logits, optimizer_idx=0)\n",
    "                    elif optimizer_idx == 1:  # discriminator update\n",
    "                        with freeze_grads(generator):\n",
    "                            real_logits = discriminator(real_images)\n",
    "                            fake_logits = discriminator(fake_images.detach())\n",
    "                            loss = gan_loss(real_logits, fake_logits, optimizer_idx=1)\n",
    "                    stat[f'loss_{optimizer_idx}'] = loss.item()\n",
    "                    loss.backward()\n",
    "                for optimizer_idx in indices:\n",
    "                    optimizers[optimizer_idx].step()\n",
    "            \n",
    "            stats_per_epoch.append(stat)\n",
    "        \n",
    "        all_stats.append(stats_per_epoch)\n",
    "        avg_stats = {key: sum(stat[key] for stat in stats_per_epoch) / len(stats_per_epoch) for key in stats_per_epoch[0]}\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {avg_stats['loss_1']:.4f}, Loss G: {avg_stats['loss_0']:.4f}\")\n",
    "        \n",
    "        # Generate images\n",
    "        generator.eval()\n",
    "        noise = torch.randn(16, noise_dim, device=device)\n",
    "        fake_images = generator(noise)\n",
    "        fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "        fake_images = (fake_images + 1) / 2\n",
    "        fake_images = fake_images.clamp(0, 1)\n",
    "        grid = fake_images.cpu().detach().numpy()\n",
    "        grid = grid.transpose(0, 2, 3, 1)\n",
    "        plt.figure(figsize=(8, 2))\n",
    "        for i in range(16):\n",
    "            plt.subplot(2, 8, i+1)\n",
    "            plt.imshow(grid[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return all_stats\n",
    "\n",
    "# Hyperparameters\n",
    "noise_dim = 100\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "lr = 0.0002\n",
    "\n",
    "def do_expe(version, indices):\n",
    "    # Seed and make deterministic\n",
    "    torch.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Data loader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    mnist = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    data_loader = DataLoader(mnist, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize models\n",
    "    generator = Generator(noise_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    # Train GAN\n",
    "    train_stats = train_gan(generator, discriminator, data_loader, noise_dim, num_epochs, lr, version=version, indices=indices)\n",
    "\n",
    "    # # plot\n",
    "    # df = pd.DataFrame(train_stats)\n",
    "    # df.plot()\n",
    "    # plt.show()\n",
    "\n",
    "do_expe(\"v0\", [0, 1])\n",
    "do_expe(\"v1\", [0, 1])\n",
    "do_expe(\"v0\", [1, 0])\n",
    "do_expe(\"v1\", [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
