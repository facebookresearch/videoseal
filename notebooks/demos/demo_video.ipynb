{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/pfz/09-videoseal/videoseal-dev\n"
     ]
    }
   ],
   "source": [
    "# run in the root of the repository\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import omegaconf\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "from videoseal.models import Wam, build_embedder, build_extractor\n",
    "from videoseal.augmentation.augmenter import Augmenter\n",
    "from videoseal.data.transforms import default_transform, normalize_img, unnormalize_img\n",
    "from videoseal.data.datasets import VideoDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and build models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:videoseal.data.datasets:Loading videos from assets/videos\n",
      "INFO:videoseal.data.datasets:Found 2 videos in assets/videos\n",
      "Processing videos in assets/videos: 100%|██████████| 2/2 [00:00<00:00, 33026.02it/s]\n",
      "INFO:videoseal.data.datasets:Total videos loaded from assets/videos: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video tensor shape: torch.Size([72, 3, 1920, 1080])\n"
     ]
    }
   ],
   "source": [
    "# load video\n",
    "video_dir = \"assets/videos\"\n",
    "video_path = \"assets/videos/sav_013754.mp4\"\n",
    "# !ffprobe -v error -show_entries stream=r_frame_rate -of default=noprint_wrappers=1:nokey=1 assets/videos/sav_013754.mp4\n",
    "\n",
    "fps = 24 // 1\n",
    "frames_per_clip = fps * 3 # 3s\n",
    "frame_step = 1\n",
    "\n",
    "vid_dataset = VideoDataset(\n",
    "    folder_paths = [video_dir], \n",
    "    frames_per_clip = frames_per_clip,\n",
    "    frame_step = frame_step\n",
    ")\n",
    "vid = vid_dataset.__getitem__(0)\n",
    "video_tensor = vid[0][0]\n",
    "video_tensor = np.transpose(video_tensor, (0, 3, 1, 2))\n",
    "video_tensor = torch.tensor(video_tensor, dtype=torch.float32)\n",
    "print(f\"Video tensor shape: {video_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.augmentation.augmenter import Augmenter\n",
    "from videoseal.modules.jnd import JND\n",
    "from videoseal.models.embedder import Embedder\n",
    "from videoseal.models.extractor import Extractor\n",
    "\n",
    "class VideoWam(nn.Module):\n",
    "    wm_threshold: float = 0.0\n",
    "    image_format: str = \"RGB\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder: Embedder,\n",
    "        detector: Extractor,\n",
    "        augmenter: Augmenter,\n",
    "        attenuation: JND = None,\n",
    "        scaling_w: float = 1.0,\n",
    "        scaling_i: float = 1.0,\n",
    "        img_size: int = 256,\n",
    "        chunk_size: int = 8,\n",
    "        step_size: int = 4,\n",
    "        device: str = device,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        WAM (watermark-anything models) model that combines an embedder, a detector, and an augmenter.\n",
    "        Embeds a message into an image and detects it as a mask.\n",
    "\n",
    "        Arguments:\n",
    "            embedder: The watermark embedder\n",
    "            detector: The watermark detector\n",
    "            augmenter: The image augmenter\n",
    "            attenuation: The JND model to attenuate the watermark distortion\n",
    "            scaling_w: The scaling factor for the watermark\n",
    "            scaling_i: The scaling factor for the image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # modules\n",
    "        self.embedder = embedder\n",
    "        self.detector = detector\n",
    "        self.augmenter = augmenter\n",
    "        self.attenuation = attenuation\n",
    "        # scalings\n",
    "        self.scaling_w = scaling_w\n",
    "        self.scaling_i = scaling_i\n",
    "        # video settings\n",
    "        self.chunk_size = chunk_size  # encode 8 imgs at a time\n",
    "        self.step_size = step_size  # propagate the wm to 4 next imgs\n",
    "        self.resize_to = transforms.Resize(img_size, antialias=True)\n",
    "        # device\n",
    "        self.device = device\n",
    "\n",
    "    def get_random_msg(self, bsz: int = 1, nb_repetitions=1) -> torch.Tensor:\n",
    "        return self.embedder.get_random_msg(bsz, nb_repetitions)  # b x k\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_inference(\n",
    "        self,\n",
    "        imgs: torch.Tensor,\n",
    "        msg: torch.Tensor = None,\n",
    "    ):\n",
    "        \"\"\" \n",
    "        Does the forward pass of the encoder only.\n",
    "        Rescale the watermark signal by a JND (just noticeable difference heatmap) that says where pixel can be changed without being noticed.\n",
    "        The watermark signal is computed on the image downsampled to 256x... pixels, and then upsampled to the original size.\n",
    "        The watermark signal is computed every step_size imgs and propagated to the next step_size imgs.\n",
    "\n",
    "        Args:\n",
    "            imgs: (torch.Tensor) Batched images with shape FxCxHxW\n",
    "            msg: (torch.Tensor) Batched messages with shape 1xL\n",
    "        \"\"\"\n",
    "        if msg is None:\n",
    "            msg = self.get_random_msg()\n",
    "\n",
    "        # encode by chunk of 8 imgs, propagate the wm to 4 next imgs\n",
    "        chunk_size = self.chunk_size  # n\n",
    "        step_size = self.step_size\n",
    "        msg = msg.repeat(chunk_size, 1).to(self.device) # 1 k -> n k\n",
    "\n",
    "        # initialize watermarked imgs\n",
    "        imgs_w = torch.zeros_like(imgs) # f 3 h w\n",
    "\n",
    "        for ii in range(0, len(imgs[::step_size]), chunk_size):\n",
    "            nimgs_in_ck = min(chunk_size, len(imgs[::step_size]) - ii)\n",
    "            start = ii*step_size\n",
    "            end = start + nimgs_in_ck * step_size\n",
    "            all_imgs_in_ck = imgs[start : end, ...].to(self.device) # f 3 h w\n",
    "\n",
    "            # choose one frame every step_size\n",
    "            imgs_in_ck = all_imgs_in_ck[::step_size] # n 3 h w\n",
    "            # downsampling with fixed short edge\n",
    "            imgs_in_ck = self.resize_to(imgs_in_ck) # n 3 wm_h wm_w\n",
    "            # deal with last chunk that may have less than chunk_size frames\n",
    "            if nimgs_in_ck < chunk_size:  \n",
    "                msg = msg[:nimgs_in_ck]\n",
    "            \n",
    "            # get deltas for the chunk, and repeat them for each frame in the chunk\n",
    "            deltas_in_ck = self.embedder(imgs_in_ck, msg) # n 3 wm_h wm_w\n",
    "            deltas_in_ck = torch.repeat_interleave(deltas_in_ck, step_size, dim=0) # f 3 wm_h wm_w\n",
    "            deltas_in_ck = deltas_in_ck[:len(all_imgs_in_ck)] # at the end of video there might be more deltas than needed\n",
    "            \n",
    "            # upsampling\n",
    "            deltas_in_ck = nn.functional.interpolate(deltas_in_ck, size=imgs.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            # create watermarked imgs\n",
    "            all_imgs_in_ck_w = self.scaling_i * all_imgs_in_ck + self.scaling_w * deltas_in_ck\n",
    "            if self.attenuation is not None:\n",
    "                all_imgs_in_ck_w = self.attenuation(all_imgs_in_ck, all_imgs_in_ck_w)\n",
    "            imgs_w[start : end, ...] = all_imgs_in_ck_w.cpu() # n 3 h w\n",
    "\n",
    "        return imgs_w\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def detect_inference(\n",
    "        self,\n",
    "        imgs: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \n",
    "        Args:\n",
    "            imgs: (torch.Tensor) Batched images with shape FxCxHxW\n",
    "        \"\"\"\n",
    "        ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train_dir='/datasets01/COCO/060817/train2014/', train_annotation_file='/datasets01/COCO/060817/annotations/instances_train2014.json', val_dir='/datasets01/COCO/060817/val2014/', val_annotation_file='/datasets01/COCO/060817/annotations/instances_val2014.json', output_dir='/checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny', embedder_config='configs/embedder.yaml', augmentation_config='configs/simple_augs.yaml', extractor_config='configs/extractor.yaml', attenuation_config='configs/attenuation.yaml', embedder_model='unet_small2', extractor_model='sam_tiny', nbits=32, img_size=256, img_size_extractor=256, attenuation='None', scaling_w=0.4, scaling_w_schedule=None, scaling_i=1.0, threshold_mask=0.6, optimizer='AdamW,lr=1e-4', optimizer_d=None, scheduler='CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5', epochs=100, batch_size=16, batch_size_eval=32, temperature=1.0, workers=8, resume_from=None, lambda_det=0.0, lambda_dec=1.0, lambda_i=0.0, lambda_d=0.0, balanced=False, total_gnorm=0.0, perceptual_loss='mse', disc_start=0, disc_num_layers=2, only_eval=False, eval_freq=5, full_eval_freq=50, saveimg_freq=5, saveckpt_freq=50, seed=0, debug_slurm=False, local_rank=0, master_port=13946, is_slurm_job=True, job_id='32284190', n_nodes=1, node_id=0, global_rank=0, world_size=8, n_gpu_per_node=8, master_addr='learnfair7487', is_master=True, multi_node=False, distributed=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4112248/4022889192.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny/checkpoint.pth\n",
      "__log__:{\"train_dir\": \"/datasets01/COCO/060817/train2014/\", \"train_annotation_file\": \"/datasets01/COCO/060817/annotations/instances_train2014.json\", \"val_dir\": \"/datasets01/COCO/060817/val2014/\", \"val_annotation_file\": \"/datasets01/COCO/060817/annotations/instances_val2014.json\", \"output_dir\": \"/checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny\", \"embedder_config\": \"configs/embedder.yaml\", \"augmentation_config\": \"configs/simple_augs.yaml\", \"extractor_config\": \"configs/extractor.yaml\", \"attenuation_config\": \"configs/attenuation.yaml\", \"embedder_model\": \"unet_small2\", \"extractor_model\": \"sam_tiny\", \"nbits\": 32, \"img_size\": 256, \"img_size_extractor\": 256, \"attenuation\": \"None\", \"scaling_w\": 0.4, \"scaling_w_schedule\": null, \"scaling_i\": 1.0, \"threshold_mask\": 0.6, \"optimizer\": \"AdamW,lr=1e-4\", \"optimizer_d\": null, \"scheduler\": \"CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5\", \"epochs\": 100, \"batch_size\": 16, \"batch_size_eval\": 32, \"temperature\": 1.0, \"workers\": 8, \"resume_from\": null, \"lambda_det\": 0.0, \"lambda_dec\": 1.0, \"lambda_i\": 0.0, \"lambda_d\": 0.0, \"balanced\": false, \"total_gnorm\": 0.0, \"perceptual_loss\": \"mse\", \"disc_start\": 0, \"disc_num_layers\": 2, \"only_eval\": false, \"eval_freq\": 5, \"full_eval_freq\": 50, \"saveimg_freq\": 5, \"saveckpt_freq\": 50, \"seed\": 0, \"debug_slurm\": false, \"local_rank\": 0, \"master_port\": 13946, \"is_slurm_job\": true, \"job_id\": \"32284190\", \"n_nodes\": 1, \"node_id\": 0, \"global_rank\": 0, \"world_size\": 8, \"n_gpu_per_node\": 8, \"master_addr\": \"learnfair7487\", \"is_master\": true, \"multi_node\": false, \"distributed\": true}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VideoWam(\n",
       "  (embedder): UnetEmbedder(\n",
       "    (unet): UNetMsg(\n",
       "      (msg_processor): MsgProcessor(\n",
       "        (msg_embeddings): Embedding(64, 64)\n",
       "      )\n",
       "      (inc): ResnetBlock(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): ChanRMSNorm()\n",
       "          (2): SiLU()\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): ChanRMSNorm()\n",
       "          (5): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (downs): ModuleList(\n",
       "        (0): DBlock(\n",
       "          (down): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): DBlock(\n",
       "          (down): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): DBlock(\n",
       "          (down): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bottleneck): BottleNeck(\n",
       "        (model): Sequential(\n",
       "          (0): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (4): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (6): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (7): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (outc): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (msg_processor): MsgProcessor(\n",
       "      (msg_embeddings): Embedding(64, 64)\n",
       "    )\n",
       "  )\n",
       "  (detector): SegmentationExtractor(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (lin2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm()\n",
       "        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (pixel_decoder): PixelDecoder(\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): Upsample(\n",
       "          (upsample_block): Sequential(\n",
       "            (0): Upsample(scale_factor=1.0, mode='bilinear')\n",
       "            (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "            (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "            (3): LayerNorm()\n",
       "            (4): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (linear): Linear(in_features=192, out_features=33, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (augmenter): Augmenter(augs=['Identity', 'JPEG', 'Resize', 'Crop', 'Rotate', 'HorizontalFlip', 'Perspective', 'GaussianBlur', 'MedianFilter', 'Brightness', 'Contrast', 'Saturation', 'Hue'], probs=tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]))\n",
       "  (resize_to): Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model_from_checkpoint(exp_dir, exp_name):\n",
    "    logfile_path = os.path.join(exp_dir, 'logs', exp_name + '.stdout')\n",
    "    ckpt_path = os.path.join(exp_dir, exp_name, 'checkpoint.pth')\n",
    "\n",
    "    # Load parameters from log file\n",
    "    with open(logfile_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if '__log__:' in line:\n",
    "                params = json.loads(line.split('__log__:')[1].strip())\n",
    "                break\n",
    "\n",
    "    # Create an argparse Namespace object from the parameters\n",
    "    args = argparse.Namespace(**params)\n",
    "    print(args)\n",
    "    \n",
    "    # Load configurations\n",
    "    for path in [args.embedder_config, args.extractor_config, args.augmentation_config]:\n",
    "        path = os.path.join(exp_dir, \"code\", path)\n",
    "    # embedder\n",
    "    embedder_cfg = omegaconf.OmegaConf.load(args.embedder_config)\n",
    "    args.embedder_model = args.embedder_model or embedder_cfg.model\n",
    "    embedder_params = embedder_cfg[args.embedder_model]\n",
    "    # extractor\n",
    "    extractor_cfg = omegaconf.OmegaConf.load(args.extractor_config)\n",
    "    args.extractor_model = args.extractor_model or extractor_cfg.model\n",
    "    extractor_params = extractor_cfg[args.extractor_model]\n",
    "    # augmenter\n",
    "    augmenter_cfg = omegaconf.OmegaConf.load(args.augmentation_config)\n",
    "    \n",
    "    # Build models\n",
    "    embedder = build_embedder(args.embedder_model, embedder_params, args.nbits)\n",
    "    extractor = build_extractor(extractor_cfg.model, extractor_params, args.img_size_extractor, args.nbits)\n",
    "    augmenter = Augmenter(**augmenter_cfg)\n",
    "    \n",
    "    # Build the complete model\n",
    "    wam = VideoWam(embedder, extractor, augmenter, \n",
    "                   scaling_w=args.scaling_w, scaling_i=args.scaling_i)\n",
    "    \n",
    "    # Load the model weights\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        wam.load_state_dict(checkpoint['model'])\n",
    "        print(\"Model loaded successfully from\", ckpt_path)\n",
    "        print(line)\n",
    "    else:\n",
    "        print(\"Checkpoint path does not exist:\", ckpt_path)\n",
    "    \n",
    "    return wam\n",
    "\n",
    "# Example usage\n",
    "exp_dir = '/checkpoint/pfz/2024_logs/0911_vseal_pw'\n",
    "exp_name = '_extractor_model=sam_tiny'\n",
    "\n",
    "wam = load_model_from_checkpoint(exp_dir, exp_name)\n",
    "wam.eval()\n",
    "wam.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attenuation_cfg = \"configs/attenuation.yaml\"\n",
    "attenuation = \"jnd_1_3\"\n",
    "attenuation_cfg = omegaconf.OmegaConf.load(attenuation_cfg)[attenuation]\n",
    "attenuation = JND(**attenuation_cfg).to(device)\n",
    "attenuation.preprocess = unnormalize_img\n",
    "attenuation.postprocess = normalize_img\n",
    "\n",
    "wam.attenuation = attenuation\n",
    "wam.scaling_w = 1.0\n",
    "wam.scaling_i = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = normalize_img(video_tensor / 255)\n",
    "vid_w = wam.embed_inference(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video fps: 24.0, duration: 3.00s, Original video fps: 24.0, duration: 13.25s\n",
      "Output video size: 4.53 MB, Original video size: 16.50 MB\n",
      "Output video size per sec: 1.51 MB, Original video size per sec: 1.25 MB\n"
     ]
    }
   ],
   "source": [
    "out_path = \"output.mp4\"\n",
    "\n",
    "video_tensor_w = unnormalize_img(vid_w)\n",
    "video_tensor_w = video_tensor_w.clamp(0, 1)\n",
    "video_tensor_w = video_tensor_w.numpy()\n",
    "video_tensor_w = 255 * np.transpose(video_tensor_w, (0, 2, 3, 1))\n",
    "\n",
    "# save_vid\n",
    "torchvision.io.write_video(out_path, video_tensor_w, fps=fps, video_codec='libx264', options={'crf': '21'})\n",
    "\n",
    "# get video fps and durations\n",
    "cap = cv2.VideoCapture(out_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "duration = frame_count / fps\n",
    "cap.release()\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ori_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "ori_frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "ori_duration = ori_frame_count / ori_fps\n",
    "cap.release()\n",
    "print(f\"Output video fps: {fps}, duration: {duration:.2f}s, Original video fps: {ori_fps}, duration: {ori_duration:.2f}s\")\n",
    "\n",
    "# get sizes\n",
    "size = os.path.getsize(out_path) / 1e6\n",
    "original_size = os.path.getsize(video_path) / 1e6\n",
    "size_per_sec = size / duration\n",
    "original_size_per_sec = original_size / ori_duration\n",
    "print(f\"Output video size: {size:.2f} MB, Original video size: {original_size:.2f} MB\")\n",
    "print(f\"Output video size per sec: {size_per_sec:.2f} MB, Original video size per sec: {original_size_per_sec:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablesign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
