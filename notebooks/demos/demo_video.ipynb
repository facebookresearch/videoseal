{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/hadyelsahar/work/code/videoseal/videoseal\n"
     ]
    }
   ],
   "source": [
    "# run in the root of the repository\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import omegaconf\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "import subprocess\n",
    "import io\n",
    "import av\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "from videoseal.models import Wam, build_embedder, build_extractor\n",
    "from videoseal.augmentation.video import compress_decompress\n",
    "from videoseal.augmentation.augmenter import Augmenter\n",
    "from videoseal.evals.metrics import psnr, ssim\n",
    "from videoseal.data.transforms import default_transform, normalize_img, unnormalize_img\n",
    "from videoseal.data.datasets import VideoDataset\n",
    "from videoseal.utils.display import save_vid, get_fps\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video\n",
    "video_dir = \"assets/videos\"\n",
    "video_path = \"assets/videos/sav_055001.mp4\"\n",
    "# !ffprobe -v error -show_entries stream=r_frame_rate -of default=noprint_wrappers=1:nokey=1 assets/videos/sav_013754.mp4\n",
    "\n",
    "fps = 24 // 1\n",
    "frames_per_clip = fps * 3 # 3s\n",
    "frame_step = 1\n",
    "\n",
    "vid_dataset = VideoDataset(\n",
    "    folder_paths = [video_dir], \n",
    "    frames_per_clip = frames_per_clip,\n",
    "    frame_step = frame_step,\n",
    "    output_resolution=(1920, 1080),\n",
    "    flatten_clips_to_frames=False,\n",
    ")\n",
    "vid = vid_dataset.__getitem__(0)\n",
    "video_tensor = vid[0]  # (T, C, H, W) \n",
    "# video_tensor = np.transpose(video_tensor, (0, 3, 1, 2))\n",
    "# video_tensor = torch.tensor(video_tensor, dtype=torch.float32)\n",
    "# print(f\"Video tensor shape: {video_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.augmentation.augmenter import Augmenter\n",
    "from videoseal.modules.jnd import JND\n",
    "from videoseal.models.embedder import Embedder\n",
    "from videoseal.models.extractor import Extractor\n",
    "from videoseal.models.video_wam import VideoWam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train_dir='/datasets01/COCO/060817/train2014/', train_annotation_file='/datasets01/COCO/060817/annotations/instances_train2014.json', val_dir='/datasets01/COCO/060817/val2014/', val_annotation_file='/datasets01/COCO/060817/annotations/instances_val2014.json', output_dir='/checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny', embedder_config='configs/embedder.yaml', augmentation_config='configs/simple_augs.yaml', extractor_config='configs/extractor.yaml', attenuation_config='configs/attenuation.yaml', embedder_model='unet_small2', extractor_model='sam_tiny', nbits=32, img_size=256, img_size_extractor=256, attenuation='None', scaling_w=0.4, scaling_w_schedule=None, scaling_i=1.0, threshold_mask=0.6, optimizer='AdamW,lr=1e-4', optimizer_d=None, scheduler='CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5', epochs=100, batch_size=16, batch_size_eval=32, temperature=1.0, workers=8, resume_from=None, lambda_det=0.0, lambda_dec=1.0, lambda_i=0.0, lambda_d=0.0, balanced=False, total_gnorm=0.0, perceptual_loss='mse', disc_start=0, disc_num_layers=2, only_eval=False, eval_freq=5, full_eval_freq=50, saveimg_freq=5, saveckpt_freq=50, seed=0, debug_slurm=False, local_rank=0, master_port=13946, is_slurm_job=True, job_id='32284190', n_nodes=1, node_id=0, global_rank=0, world_size=8, n_gpu_per_node=8, master_addr='learnfair7487', is_master=True, multi_node=False, distributed=True)\n",
      "Model loaded successfully from /checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny/checkpoint.pth\n",
      "__log__:{\"train_dir\": \"/datasets01/COCO/060817/train2014/\", \"train_annotation_file\": \"/datasets01/COCO/060817/annotations/instances_train2014.json\", \"val_dir\": \"/datasets01/COCO/060817/val2014/\", \"val_annotation_file\": \"/datasets01/COCO/060817/annotations/instances_val2014.json\", \"output_dir\": \"/checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny\", \"embedder_config\": \"configs/embedder.yaml\", \"augmentation_config\": \"configs/simple_augs.yaml\", \"extractor_config\": \"configs/extractor.yaml\", \"attenuation_config\": \"configs/attenuation.yaml\", \"embedder_model\": \"unet_small2\", \"extractor_model\": \"sam_tiny\", \"nbits\": 32, \"img_size\": 256, \"img_size_extractor\": 256, \"attenuation\": \"None\", \"scaling_w\": 0.4, \"scaling_w_schedule\": null, \"scaling_i\": 1.0, \"threshold_mask\": 0.6, \"optimizer\": \"AdamW,lr=1e-4\", \"optimizer_d\": null, \"scheduler\": \"CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5\", \"epochs\": 100, \"batch_size\": 16, \"batch_size_eval\": 32, \"temperature\": 1.0, \"workers\": 8, \"resume_from\": null, \"lambda_det\": 0.0, \"lambda_dec\": 1.0, \"lambda_i\": 0.0, \"lambda_d\": 0.0, \"balanced\": false, \"total_gnorm\": 0.0, \"perceptual_loss\": \"mse\", \"disc_start\": 0, \"disc_num_layers\": 2, \"only_eval\": false, \"eval_freq\": 5, \"full_eval_freq\": 50, \"saveimg_freq\": 5, \"saveckpt_freq\": 50, \"seed\": 0, \"debug_slurm\": false, \"local_rank\": 0, \"master_port\": 13946, \"is_slurm_job\": true, \"job_id\": \"32284190\", \"n_nodes\": 1, \"node_id\": 0, \"global_rank\": 0, \"world_size\": 8, \"n_gpu_per_node\": 8, \"master_addr\": \"learnfair7487\", \"is_master\": true, \"multi_node\": false, \"distributed\": true}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1171740/4022889192.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VideoWam(\n",
       "  (embedder): UnetEmbedder(\n",
       "    (unet): UNetMsg(\n",
       "      (msg_processor): MsgProcessor(\n",
       "        (msg_embeddings): Embedding(64, 64)\n",
       "      )\n",
       "      (inc): ResnetBlock(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): ChanRMSNorm()\n",
       "          (2): SiLU()\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): ChanRMSNorm()\n",
       "          (5): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (downs): ModuleList(\n",
       "        (0): DBlock(\n",
       "          (down): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): DBlock(\n",
       "          (down): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): DBlock(\n",
       "          (down): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bottleneck): BottleNeck(\n",
       "        (model): Sequential(\n",
       "          (0): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (4): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (6): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (7): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (outc): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (msg_processor): MsgProcessor(\n",
       "      (msg_embeddings): Embedding(64, 64)\n",
       "    )\n",
       "  )\n",
       "  (detector): SegmentationExtractor(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (lin2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm()\n",
       "        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (pixel_decoder): PixelDecoder(\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): Upsample(\n",
       "          (upsample_block): Sequential(\n",
       "            (0): Upsample(scale_factor=1.0, mode='bilinear')\n",
       "            (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "            (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "            (3): LayerNorm()\n",
       "            (4): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (linear): Linear(in_features=192, out_features=33, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (augmenter): Augmenter(augs=['Identity', 'JPEG', 'Resize', 'Crop', 'Rotate', 'HorizontalFlip', 'Perspective', 'GaussianBlur', 'MedianFilter', 'Brightness', 'Contrast', 'Saturation', 'Hue'], probs=tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]))\n",
       "  (resize_to): Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model_from_checkpoint(exp_dir, exp_name):\n",
    "    logfile_path = os.path.join(exp_dir, 'logs', exp_name + '.stdout')\n",
    "    ckpt_path = os.path.join(exp_dir, exp_name, 'checkpoint.pth')\n",
    "\n",
    "    # Load parameters from log file\n",
    "    with open(logfile_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if '__log__:' in line:\n",
    "                params = json.loads(line.split('__log__:')[1].strip())\n",
    "                break\n",
    "\n",
    "    # Create an argparse Namespace object from the parameters\n",
    "    args = argparse.Namespace(**params)\n",
    "    print(args)\n",
    "    \n",
    "    # Load configurations\n",
    "    for path in [args.embedder_config, args.extractor_config, args.augmentation_config]:\n",
    "        path = os.path.join(exp_dir, \"code\", path)\n",
    "    # embedder\n",
    "    embedder_cfg = omegaconf.OmegaConf.load(args.embedder_config)\n",
    "    args.embedder_model = args.embedder_model or embedder_cfg.model\n",
    "    embedder_params = embedder_cfg[args.embedder_model]\n",
    "    # extractor\n",
    "    extractor_cfg = omegaconf.OmegaConf.load(args.extractor_config)\n",
    "    args.extractor_model = args.extractor_model or extractor_cfg.model\n",
    "    extractor_params = extractor_cfg[args.extractor_model]\n",
    "    # augmenter\n",
    "    augmenter_cfg = omegaconf.OmegaConf.load(args.augmentation_config)\n",
    "    \n",
    "    # Build models\n",
    "    embedder = build_embedder(args.embedder_model, embedder_params, args.nbits)\n",
    "    extractor = build_extractor(extractor_cfg.model, extractor_params, args.img_size_extractor, args.nbits)\n",
    "    augmenter = Augmenter(**augmenter_cfg)\n",
    "    \n",
    "    # Build the complete model\n",
    "    wam = VideoWam(embedder, extractor, augmenter, \n",
    "                   scaling_w=args.scaling_w, scaling_i=args.scaling_i)\n",
    "    \n",
    "    # Load the model weights\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        wam.load_state_dict(checkpoint['model'])\n",
    "        print(\"Model loaded successfully from\", ckpt_path)\n",
    "        print(line)\n",
    "    else:\n",
    "        print(\"Checkpoint path does not exist:\", ckpt_path)\n",
    "    \n",
    "    return wam\n",
    "\n",
    "# Example usage\n",
    "exp_dir = '/checkpoint/pfz/2024_logs/0911_vseal_pw'\n",
    "exp_name = '_extractor_model=sam_tiny'\n",
    "\n",
    "wam = load_model_from_checkpoint(exp_dir, exp_name)\n",
    "wam.eval()\n",
    "wam.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attenuation_cfg = \"configs/attenuation.yaml\"\n",
    "attenuation = \"jnd_1_3\"\n",
    "attenuation_cfg = omegaconf.OmegaConf.load(attenuation_cfg)[attenuation]\n",
    "attenuation = JND(**attenuation_cfg).to(device)\n",
    "attenuation.preprocess = unnormalize_img\n",
    "attenuation.postprocess = normalize_img\n",
    "\n",
    "wam.attenuation = attenuation\n",
    "wam.attenuation = attenuation\n",
    "wam.scaling_w = 3.0\n",
    "wam.scaling_i = 1.0\n",
    "\n",
    "# wam.attenuation = None\n",
    "# wam.scaling_w = 0.4\n",
    "# wam.scaling_i = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = wam.get_random_msg()\n",
    "\n",
    "# vid = normalize_img(video_tensor / 255)\n",
    "vid = normalize_img(video_tensor)\n",
    "\n",
    "vid = vid.to(device)\n",
    "msg = msg.to(device)\n",
    "vid_w = wam.embed_inference(vid, msg)\n",
    "preds = wam.detect_inference(vid_w)\n",
    "\n",
    "psnr_val = psnr(vid_w, vid).mean().item()\n",
    "ssim_val = ssim(vid_w, vid).mean().item()\n",
    "bit_acc = (msg == preds).float().mean().item()\n",
    "print(\"Before compression\")\n",
    "print(f\"PSNR: {psnr_val:.2f}, SSIM: {ssim_val:.2f}, Bit accuracy: {bit_acc:.2f}\")\n",
    "\n",
    "# compress and decompress\n",
    "print(\"After compression\")\n",
    "for crf in [15, 25, 50]:\n",
    "    vid_w_comp = compress_decompress(vid_w, codec='libx264', crf=crf)\n",
    "    preds_comp = wam.detect_inference(vid_w_comp)\n",
    "    bit_acc_comp = (msg == preds_comp).float().mean().item()\n",
    "\n",
    "    psnr_val = psnr(vid_w_comp, vid).mean().item()\n",
    "    ssim_val = ssim(vid_w_comp, vid).mean().item()\n",
    "    print(f\"CRF: {crf}, PSNR: {psnr_val:.2f}, SSIM: {ssim_val:.2f}, Bit accuracy: {bit_acc_comp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ffmpeg -i output_w.mp4 -i  output_o.mp4 -filter_complex ssim -f null - 2>&1 | grep \" SSIM \"\n",
    "# # https://stackoverflow.com/questions/62061410/can-someone-help-me-to-install-the-netflixs-vmaf-library-in-ubuntu\n",
    "# !ffmpeg -i output_w.mp4 -i  output_o.mp4 -filter_complex libvmaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.utils.display import save_vid\n",
    "\n",
    "out_path = \"./outputs/\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "# Save videos in the specified output directory\n",
    "\n",
    "raw_path = os.path.join(out_path, \"output_raw.mp4\")\n",
    "wmed_path = os.path.join(out_path, \"output_wmed.mp4\")\n",
    "\n",
    "save_vid(vid, raw_path, fps)\n",
    "save_vid(vid_w, wmed_path, fps)\n",
    "save_vid(vid - vid_w, os.path.join(out_path, \"output_wm.mp4\"), fps)\n",
    "\n",
    "# get video fps and durations\n",
    "fps, frame_count = get_fps(raw_path)\n",
    "duration = frame_count / fps\n",
    "\n",
    "ori_fps, ori_frame_count = get_fps(wmed_path)\n",
    "ori_duration = ori_frame_count / ori_fps\n",
    "print(f\"Output video fps: {fps}, duration: {duration:.2f}s, Original video fps: {ori_fps}, duration: {ori_duration:.2f}s\")\n",
    "\n",
    "# get sizes\n",
    "size = os.path.getsize(raw_path) / 1e6\n",
    "original_size = os.path.getsize(wmed_path) / 1e6\n",
    "size_per_sec = size / duration\n",
    "original_size_per_sec = original_size / ori_duration\n",
    "print(f\"Output video size: {size:.2f} MB, Original video size: {original_size:.2f} MB\")\n",
    "print(f\"Output video size per sec: {size_per_sec:.2f} MB, Original video size per sec: {original_size_per_sec:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_video.shape -> torch.Size([t, c, h, w])\n",
    "def plot_images_from_video_tensor(video_tensor, n=5):\n",
    "    # prepare images\n",
    "    video_tensor = video_tensor.clamp(0, 1).permute(0, 2, 3, 1)  # t c h w -> t w h c\n",
    "    video_tensor = video_tensor.cpu().numpy() \n",
    "    # plot\n",
    "    indexes = np.linspace(0, len(video_tensor)-1, n).astype(int)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n*3, 3))\n",
    "    for ii, idx in enumerate(indexes):\n",
    "        axes[ii].imshow(video_tensor[idx])\n",
    "        axes[ii].axis('off')\n",
    "        axes[ii].set_title(f\"Frame {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "plot_images_from_video_tensor(vid_w-vid, n=5)\n",
    "plot_images_from_video_tensor(video_tensor, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablesign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
