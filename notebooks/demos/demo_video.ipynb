{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/pfz/09-videoseal/videoseal-dev\n"
     ]
    }
   ],
   "source": [
    "# run in the root of the repository\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import omegaconf\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "import subprocess\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "from videoseal.models import Wam, build_embedder, build_extractor\n",
    "from videoseal.augmentation.augmenter import Augmenter\n",
    "from videoseal.evals.metrics import psnr, ssim\n",
    "from videoseal.data.transforms import default_transform, normalize_img, unnormalize_img\n",
    "from videoseal.data.datasets import VideoDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:videoseal.data.datasets:Loading videos from assets/videos\n",
      "INFO:videoseal.data.datasets:Found 5 videos in assets/videos\n",
      "Processing videos in assets/videos: 100%|██████████| 5/5 [00:00<00:00, 74631.74it/s]\n",
      "INFO:videoseal.data.datasets:Total videos loaded from assets/videos: 5\n"
     ]
    }
   ],
   "source": [
    "# load video\n",
    "video_dir = \"assets/videos\"\n",
    "video_path = \"assets/videos/sav_013754.mp4\"\n",
    "# !ffprobe -v error -show_entries stream=r_frame_rate -of default=noprint_wrappers=1:nokey=1 assets/videos/sav_013754.mp4\n",
    "\n",
    "fps = 24 // 1\n",
    "frames_per_clip = fps * 3 # 3s\n",
    "frame_step = 1\n",
    "\n",
    "vid_dataset = VideoDataset(\n",
    "    folder_paths = [video_dir], \n",
    "    frames_per_clip = frames_per_clip,\n",
    "    frame_step = frame_step,\n",
    "    output_resolution=(1920, 1080),\n",
    ")\n",
    "vid = vid_dataset.__getitem__(0)\n",
    "video_tensor = vid[0][0]  # (T, C, H, W)\n",
    "# video_tensor = np.transpose(video_tensor, (0, 3, 1, 2))\n",
    "# video_tensor = torch.tensor(video_tensor, dtype=torch.float32)\n",
    "# print(f\"Video tensor shape: {video_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.augmentation.augmenter import Augmenter\n",
    "from videoseal.modules.jnd import JND\n",
    "from videoseal.models.embedder import Embedder\n",
    "from videoseal.models.extractor import Extractor\n",
    "\n",
    "class VideoWam(nn.Module):\n",
    "    wm_threshold: float = 0.0\n",
    "    image_format: str = \"RGB\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder: Embedder,\n",
    "        detector: Extractor,\n",
    "        augmenter: Augmenter,\n",
    "        attenuation: JND = None,\n",
    "        scaling_w: float = 1.0,\n",
    "        scaling_i: float = 1.0,\n",
    "        img_size: int = 256,\n",
    "        chunk_size: int = 8,\n",
    "        step_size: int = 4,\n",
    "        device: str = device,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        WAM (watermark-anything models) model that combines an embedder, a detector, and an augmenter.\n",
    "        Embeds a message into an image and detects it as a mask.\n",
    "\n",
    "        Arguments:\n",
    "            embedder: The watermark embedder\n",
    "            detector: The watermark detector\n",
    "            augmenter: The image augmenter\n",
    "            attenuation: The JND model to attenuate the watermark distortion\n",
    "            scaling_w: The scaling factor for the watermark\n",
    "            scaling_i: The scaling factor for the image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # modules\n",
    "        self.embedder = embedder\n",
    "        self.detector = detector\n",
    "        self.augmenter = augmenter\n",
    "        self.attenuation = attenuation\n",
    "        # scalings\n",
    "        self.scaling_w = scaling_w\n",
    "        self.scaling_i = scaling_i\n",
    "        # video settings\n",
    "        self.chunk_size = chunk_size  # encode 8 imgs at a time\n",
    "        self.step_size = step_size  # propagate the wm to 4 next imgs\n",
    "        self.resize_to = transforms.Resize((img_size, img_size), antialias=True)\n",
    "        # device\n",
    "        self.device = device\n",
    "\n",
    "    def get_random_msg(self, bsz: int = 1, nb_repetitions=1) -> torch.Tensor:\n",
    "        return self.embedder.get_random_msg(bsz, nb_repetitions)  # b x k\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_inference(\n",
    "        self,\n",
    "        imgs: torch.Tensor,\n",
    "        msg: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "        Does the forward pass of the encoder only.\n",
    "        Rescale the watermark signal by a JND (just noticeable difference heatmap) that says where pixel can be changed without being noticed.\n",
    "        The watermark signal is computed on the image downsampled to 256x... pixels, and then upsampled to the original size.\n",
    "        The watermark signal is computed every step_size imgs and propagated to the next step_size imgs.\n",
    "\n",
    "        Args:\n",
    "            imgs: (torch.Tensor) Batched images with shape FxCxHxW\n",
    "            msg: (torch.Tensor) Batched messages with shape 1xL\n",
    "        \n",
    "        Returns:\n",
    "            imgs_w: (torch.Tensor) Batched watermarked images with shape FxCxHxW\n",
    "        \"\"\"\n",
    "        if msg is None:\n",
    "            msg = self.get_random_msg()\n",
    "\n",
    "        # encode by chunk of 8 imgs, propagate the wm to 4 next imgs\n",
    "        chunk_size = self.chunk_size  # n\n",
    "        step_size = self.step_size\n",
    "        msg = msg.repeat(chunk_size, 1).to(self.device) # 1 k -> n k\n",
    "\n",
    "        # initialize watermarked imgs\n",
    "        imgs_w = torch.zeros_like(imgs) # f 3 h w\n",
    "\n",
    "        for ii in range(0, len(imgs[::step_size]), chunk_size):\n",
    "            nimgs_in_ck = min(chunk_size, len(imgs[::step_size]) - ii)\n",
    "            start = ii*step_size\n",
    "            end = start + nimgs_in_ck * step_size\n",
    "            all_imgs_in_ck = imgs[start : end, ...].to(self.device) # f 3 h w\n",
    "\n",
    "            # choose one frame every step_size\n",
    "            imgs_in_ck = all_imgs_in_ck[::step_size] # n 3 h w\n",
    "            # downsampling with fixed short edge\n",
    "            imgs_in_ck = self.resize_to(imgs_in_ck) # n 3 wm_h wm_w\n",
    "            # deal with last chunk that may have less than chunk_size frames\n",
    "            if nimgs_in_ck < chunk_size:  \n",
    "                msg = msg[:nimgs_in_ck]\n",
    "            \n",
    "            # get deltas for the chunk, and repeat them for each frame in the chunk\n",
    "            deltas_in_ck = self.embedder(imgs_in_ck, msg) # n 3 wm_h wm_w\n",
    "            deltas_in_ck = torch.repeat_interleave(deltas_in_ck, step_size, dim=0) # f 3 wm_h wm_w\n",
    "            deltas_in_ck = deltas_in_ck[:len(all_imgs_in_ck)] # at the end of video there might be more deltas than needed\n",
    "            \n",
    "            # upsampling\n",
    "            deltas_in_ck = nn.functional.interpolate(deltas_in_ck, size=imgs.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            # create watermarked imgs\n",
    "            all_imgs_in_ck_w = self.scaling_i * all_imgs_in_ck + self.scaling_w * deltas_in_ck\n",
    "            if self.attenuation is not None:\n",
    "                all_imgs_in_ck_w = self.attenuation(all_imgs_in_ck, all_imgs_in_ck_w)\n",
    "            imgs_w[start : end, ...] = all_imgs_in_ck_w.cpu() # n 3 h w\n",
    "\n",
    "        return imgs_w\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def detect_inference(\n",
    "        self,\n",
    "        frames: torch.Tensor,\n",
    "        aggregation: str = \"avg\",\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Does the forward pass of the detector only.\n",
    "        Rescale the image to 256x... pixels, and then compute the mask and the message.\n",
    "        \n",
    "        Args:\n",
    "            imgs: (torch.Tensor) Batched images with shape FxCxHxW\n",
    "        \"\"\"\n",
    "        frames = self.resize_to(frames)\n",
    "        chunksize = 16  # n\n",
    "        all_preds = []\n",
    "        for ii in range(0, len(frames), chunksize):\n",
    "            nframes_in_ck = min(chunksize, len(frames) - ii)\n",
    "            preds = self.detector(\n",
    "                frames[ii:ii+nframes_in_ck].to(self.device)\n",
    "            ).cpu()\n",
    "            all_preds.append(preds)  # n k ..\n",
    "        preds = torch.cat(all_preds, dim=0) # f k ..\n",
    "        mask_preds = preds[:, 0:1]  # b ..\n",
    "        bit_preds = preds[:, 1:]  # b k ..\n",
    "\n",
    "        if aggregation is None:\n",
    "            decoded_msg = bit_preds\n",
    "        elif aggregation == \"avg\":\n",
    "            decoded_msg = bit_preds.mean(dim=0)\n",
    "        elif aggregation == \"weighted_avg\":\n",
    "            decoded_msg = (bit_preds * bit_preds.abs()).mean(dim=0) # b k -> k\n",
    "        msg = (decoded_msg > 0).squeeze().numpy()\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train_dir='/datasets01/COCO/060817/train2014/', train_annotation_file='/datasets01/COCO/060817/annotations/instances_train2014.json', val_dir='/datasets01/COCO/060817/val2014/', val_annotation_file='/datasets01/COCO/060817/annotations/instances_val2014.json', output_dir='/checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny', embedder_config='configs/embedder.yaml', augmentation_config='configs/simple_augs.yaml', extractor_config='configs/extractor.yaml', attenuation_config='configs/attenuation.yaml', embedder_model='unet_small2', extractor_model='sam_tiny', nbits=32, img_size=256, img_size_extractor=256, attenuation='None', scaling_w=0.4, scaling_w_schedule=None, scaling_i=1.0, threshold_mask=0.6, optimizer='AdamW,lr=1e-4', optimizer_d=None, scheduler='CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5', epochs=100, batch_size=16, batch_size_eval=32, temperature=1.0, workers=8, resume_from=None, lambda_det=0.0, lambda_dec=1.0, lambda_i=0.0, lambda_d=0.0, balanced=False, total_gnorm=0.0, perceptual_loss='mse', disc_start=0, disc_num_layers=2, only_eval=False, eval_freq=5, full_eval_freq=50, saveimg_freq=5, saveckpt_freq=50, seed=0, debug_slurm=False, local_rank=0, master_port=13946, is_slurm_job=True, job_id='32284190', n_nodes=1, node_id=0, global_rank=0, world_size=8, n_gpu_per_node=8, master_addr='learnfair7487', is_master=True, multi_node=False, distributed=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26847/4022889192.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny/checkpoint.pth\n",
      "__log__:{\"train_dir\": \"/datasets01/COCO/060817/train2014/\", \"train_annotation_file\": \"/datasets01/COCO/060817/annotations/instances_train2014.json\", \"val_dir\": \"/datasets01/COCO/060817/val2014/\", \"val_annotation_file\": \"/datasets01/COCO/060817/annotations/instances_val2014.json\", \"output_dir\": \"/checkpoint/pfz/2024_logs/0911_vseal_pw/_extractor_model=sam_tiny\", \"embedder_config\": \"configs/embedder.yaml\", \"augmentation_config\": \"configs/simple_augs.yaml\", \"extractor_config\": \"configs/extractor.yaml\", \"attenuation_config\": \"configs/attenuation.yaml\", \"embedder_model\": \"unet_small2\", \"extractor_model\": \"sam_tiny\", \"nbits\": 32, \"img_size\": 256, \"img_size_extractor\": 256, \"attenuation\": \"None\", \"scaling_w\": 0.4, \"scaling_w_schedule\": null, \"scaling_i\": 1.0, \"threshold_mask\": 0.6, \"optimizer\": \"AdamW,lr=1e-4\", \"optimizer_d\": null, \"scheduler\": \"CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5\", \"epochs\": 100, \"batch_size\": 16, \"batch_size_eval\": 32, \"temperature\": 1.0, \"workers\": 8, \"resume_from\": null, \"lambda_det\": 0.0, \"lambda_dec\": 1.0, \"lambda_i\": 0.0, \"lambda_d\": 0.0, \"balanced\": false, \"total_gnorm\": 0.0, \"perceptual_loss\": \"mse\", \"disc_start\": 0, \"disc_num_layers\": 2, \"only_eval\": false, \"eval_freq\": 5, \"full_eval_freq\": 50, \"saveimg_freq\": 5, \"saveckpt_freq\": 50, \"seed\": 0, \"debug_slurm\": false, \"local_rank\": 0, \"master_port\": 13946, \"is_slurm_job\": true, \"job_id\": \"32284190\", \"n_nodes\": 1, \"node_id\": 0, \"global_rank\": 0, \"world_size\": 8, \"n_gpu_per_node\": 8, \"master_addr\": \"learnfair7487\", \"is_master\": true, \"multi_node\": false, \"distributed\": true}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VideoWam(\n",
       "  (embedder): UnetEmbedder(\n",
       "    (unet): UNetMsg(\n",
       "      (msg_processor): MsgProcessor(\n",
       "        (msg_embeddings): Embedding(64, 64)\n",
       "      )\n",
       "      (inc): ResnetBlock(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): ChanRMSNorm()\n",
       "          (2): SiLU()\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): ChanRMSNorm()\n",
       "          (5): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (downs): ModuleList(\n",
       "        (0): DBlock(\n",
       "          (down): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): DBlock(\n",
       "          (down): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): DBlock(\n",
       "          (down): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bottleneck): BottleNeck(\n",
       "        (model): Sequential(\n",
       "          (0): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (4): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (6): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (7): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (outc): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (msg_processor): MsgProcessor(\n",
       "      (msg_embeddings): Embedding(64, 64)\n",
       "    )\n",
       "  )\n",
       "  (detector): SegmentationExtractor(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (lin2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm()\n",
       "        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (pixel_decoder): PixelDecoder(\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): Upsample(\n",
       "          (upsample_block): Sequential(\n",
       "            (0): Upsample(scale_factor=1.0, mode='bilinear')\n",
       "            (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "            (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "            (3): LayerNorm()\n",
       "            (4): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (linear): Linear(in_features=192, out_features=33, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (augmenter): Augmenter(augs=['Identity', 'JPEG', 'Resize', 'Crop', 'Rotate', 'HorizontalFlip', 'Perspective', 'GaussianBlur', 'MedianFilter', 'Brightness', 'Contrast', 'Saturation', 'Hue'], probs=tensor([0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]))\n",
       "  (resize_to): Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model_from_checkpoint(exp_dir, exp_name):\n",
    "    logfile_path = os.path.join(exp_dir, 'logs', exp_name + '.stdout')\n",
    "    ckpt_path = os.path.join(exp_dir, exp_name, 'checkpoint.pth')\n",
    "\n",
    "    # Load parameters from log file\n",
    "    with open(logfile_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if '__log__:' in line:\n",
    "                params = json.loads(line.split('__log__:')[1].strip())\n",
    "                break\n",
    "\n",
    "    # Create an argparse Namespace object from the parameters\n",
    "    args = argparse.Namespace(**params)\n",
    "    print(args)\n",
    "    \n",
    "    # Load configurations\n",
    "    for path in [args.embedder_config, args.extractor_config, args.augmentation_config]:\n",
    "        path = os.path.join(exp_dir, \"code\", path)\n",
    "    # embedder\n",
    "    embedder_cfg = omegaconf.OmegaConf.load(args.embedder_config)\n",
    "    args.embedder_model = args.embedder_model or embedder_cfg.model\n",
    "    embedder_params = embedder_cfg[args.embedder_model]\n",
    "    # extractor\n",
    "    extractor_cfg = omegaconf.OmegaConf.load(args.extractor_config)\n",
    "    args.extractor_model = args.extractor_model or extractor_cfg.model\n",
    "    extractor_params = extractor_cfg[args.extractor_model]\n",
    "    # augmenter\n",
    "    augmenter_cfg = omegaconf.OmegaConf.load(args.augmentation_config)\n",
    "    \n",
    "    # Build models\n",
    "    embedder = build_embedder(args.embedder_model, embedder_params, args.nbits)\n",
    "    extractor = build_extractor(extractor_cfg.model, extractor_params, args.img_size_extractor, args.nbits)\n",
    "    augmenter = Augmenter(**augmenter_cfg)\n",
    "    \n",
    "    # Build the complete model\n",
    "    wam = VideoWam(embedder, extractor, augmenter, \n",
    "                   scaling_w=args.scaling_w, scaling_i=args.scaling_i)\n",
    "    \n",
    "    # Load the model weights\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        wam.load_state_dict(checkpoint['model'])\n",
    "        print(\"Model loaded successfully from\", ckpt_path)\n",
    "        print(line)\n",
    "    else:\n",
    "        print(\"Checkpoint path does not exist:\", ckpt_path)\n",
    "    \n",
    "    return wam\n",
    "\n",
    "# Example usage\n",
    "exp_dir = '/checkpoint/pfz/2024_logs/0911_vseal_pw'\n",
    "exp_name = '_extractor_model=sam_tiny'\n",
    "\n",
    "wam = load_model_from_checkpoint(exp_dir, exp_name)\n",
    "wam.eval()\n",
    "wam.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attenuation_cfg = \"configs/attenuation.yaml\"\n",
    "attenuation = \"jnd_1_3\"\n",
    "attenuation_cfg = omegaconf.OmegaConf.load(attenuation_cfg)[attenuation]\n",
    "attenuation = JND(**attenuation_cfg).to(device)\n",
    "attenuation.preprocess = unnormalize_img\n",
    "attenuation.postprocess = normalize_img\n",
    "\n",
    "wam.attenuation = attenuation\n",
    "wam.scaling_w = 2.0\n",
    "wam.scaling_i = 1.0\n",
    "\n",
    "# wam.attenuation = None\n",
    "# wam.scaling_w = 0.4\n",
    "# wam.scaling_i = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 38.48, SSIM: 0.99, Bit accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "msg = wam.get_random_msg()\n",
    "\n",
    "vid = normalize_img(video_tensor / 255)\n",
    "vid_w = wam.embed_inference(vid, msg)\n",
    "preds = wam.detect_inference(vid_w)\n",
    "\n",
    "psnr_val = psnr(vid_w, vid).mean().item()\n",
    "ssim_val = ssim(vid_w, vid).mean().item()\n",
    "bit_acc = (msg == preds).float().mean().item()\n",
    "print(f\"PSNR: {psnr_val:.2f}, SSIM: {ssim_val:.2f}, Bit accuracy: {bit_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ffmpeg -i output_w.mp4 -i  output_o.mp4 -filter_complex ssim -f null - 2>&1 | grep \" SSIM \"\n",
    "# # https://stackoverflow.com/questions/62061410/can-someone-help-me-to-install-the-netflixs-vmaf-library-in-ubuntu\n",
    "# !ffmpeg -i output_w.mp4 -i  output_o.mp4 -filter_complex libvmaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video fps: 24.0, duration: 3.00s, Original video fps: 24.0, duration: 13.25s\n",
      "Output video size: 4.72 MB, Original video size: 16.50 MB\n",
      "Output video size per sec: 1.57 MB, Original video size per sec: 1.25 MB\n"
     ]
    }
   ],
   "source": [
    "def save_vid(vid, out_path, fps):\n",
    "    video_tensor_w = unnormalize_img(vid)\n",
    "    video_tensor_w = video_tensor_w.clamp(0, 1)\n",
    "    video_tensor_w = video_tensor_w.numpy()\n",
    "    video_tensor_w = 255 * np.transpose(video_tensor_w, (0, 2, 3, 1))\n",
    "    torchvision.io.write_video(out_path, video_tensor_w, fps=fps, video_codec='libx264', options={'crf': '21'})\n",
    "\n",
    "out_path = \"output_w.mp4\"\n",
    "out_path_ori = \"output_o.mp4\"\n",
    "save_vid(vid_w, out_path, fps)\n",
    "save_vid(vid, out_path_ori, fps)\n",
    "\n",
    "# get video fps and durations\n",
    "cap = cv2.VideoCapture(out_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "duration = frame_count / fps\n",
    "cap.release()\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ori_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "ori_frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "ori_duration = ori_frame_count / ori_fps\n",
    "cap.release()\n",
    "print(f\"Output video fps: {fps}, duration: {duration:.2f}s, Original video fps: {ori_fps}, duration: {ori_duration:.2f}s\")\n",
    "\n",
    "# get sizes\n",
    "size = os.path.getsize(out_path) / 1e6\n",
    "original_size = os.path.getsize(video_path) / 1e6\n",
    "size_per_sec = size / duration\n",
    "original_size_per_sec = original_size / ori_duration\n",
    "print(f\"Output video size: {size:.2f} MB, Original video size: {original_size:.2f} MB\")\n",
    "print(f\"Output video size per sec: {size_per_sec:.2f} MB, Original video size per sec: {original_size_per_sec:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, rawvideo, from 'pipe:0':\n",
      "  Duration: N/A, start: 0.000000, bitrate: 1244160 kb/s\n",
      "  Stream #0:0: Video: rawvideo (RGB[24] / 0x18424752), rgb24, 1920x1080, 1244160 kb/s, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (rawvideo (native) -> h264 (libx264))\n",
      "[libx264 @ 0x5653d632d400] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x5653d632d400] profile High 4:4:4 Intra, level 4.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x5653d632d400] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=1 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=34 lookahead_threads=5 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=1 keyint_min=1 scenecut=40 intra_refresh=0 rc=crf mbtree=0 crf=18.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, matroska, to 'pipe:1':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (H264 / 0x34363248), yuv444p(tv, progressive), 1920x1080, q=2-31, 24 fps, 1k tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   70 fps= 41 q=-1.0 Lsize=   37835kB time=00:00:02.87 bitrate=107769.3kbits/s dup=0 drop=2 speed=1.67x    \n",
      "video:37833kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.005952%\n",
      "[libx264 @ 0x5653d632d400] frame I:70    Avg QP:29.53  size:553431\n",
      "[libx264 @ 0x5653d632d400] mb I  I16..4: 22.4% 36.8% 40.8%\n",
      "[libx264 @ 0x5653d632d400] 8x8 transform intra:36.8%\n",
      "[libx264 @ 0x5653d632d400] coded y,u,v intra: 97.6% 38.2% 31.3%\n",
      "[libx264 @ 0x5653d632d400] i16 v,h,dc,p:  2% 49% 38% 11%\n",
      "[libx264 @ 0x5653d632d400] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  3% 65% 13%  2%  3%  2%  4%  2%  7%\n",
      "[libx264 @ 0x5653d632d400] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu:  4% 75%  7%  2%  2%  1%  3%  1%  5%\n",
      "[libx264 @ 0x5653d632d400] kb/s:106258.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 3, 1920, 1080])\n",
      "torch.Size([70, 1080, 1920, 3])\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "\n",
    "def compress_video_with_ffmpeg(video_tensor, fps=24, codec='libx264', crf=18):\n",
    "    \"\"\"\n",
    "    Compress a video tensor using FFmpeg.\n",
    "    Args:\n",
    "        video_tensor (torch.Tensor): Video tensor of shape (T, C, H, W), normalized\n",
    "        codec (str): FFmpeg codec. Defaults to 'libx264'.\n",
    "        crf (int): Quality setting for the codec. Lower values result in higher quality and larger files. Defaults to 18.\n",
    "    Returns:\n",
    "        torch.Tensor: Compressed video tensor.\n",
    "    \"\"\"\n",
    "    # normalize video tensor and convert to np uint8 array\n",
    "    video_tensor = unnormalize_img(video_tensor)\n",
    "    video_tensor = video_tensor.clamp(0, 1)\n",
    "    video_array = (video_tensor * 255).to(torch.uint8).numpy()\n",
    "    # create input pipe\n",
    "    input_pipe = io.BytesIO()\n",
    "    for frame in video_array:\n",
    "        frame_bytes = frame.tobytes()\n",
    "        input_pipe.write(frame_bytes)\n",
    "    # set up FFmpeg command\n",
    "    command = [\n",
    "        '/bin/ffmpeg',\n",
    "        '-f', 'rawvideo',\n",
    "        '-pix_fmt', 'rgb24',\n",
    "        '-s', '{}x{}'.format(video_tensor.shape[-2], video_tensor.shape[-1]),  # Width x Height\n",
    "        '-i', 'pipe:0',  # Input from stdin\n",
    "        '-c:v', codec,\n",
    "        '-r', str(fps),\n",
    "        '-crf', str(crf),\n",
    "        '-g', '1',  # Set GOP size to 1\n",
    "        '-keyint_min', '1',  # Set keyframe interval to 1\n",
    "        '-f', 'matroska',  # Use Matroska container format\n",
    "        'pipe:1'\n",
    "    ]\n",
    "    # run FFmpeg\n",
    "    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    output, _ = process.communicate(input=input_pipe.getvalue())\n",
    "    # read the output using PyAV\n",
    "    with io.BytesIO(output) as f:\n",
    "        container = av.open(f)\n",
    "        stream = next(s for s in container.streams if s.type == 'video')\n",
    "        compressed_video = []\n",
    "        for packet in container.demux(stream):\n",
    "            for frame in packet.decode():\n",
    "                img = frame.to_ndarray(format='rgb24')\n",
    "                compressed_video.append(torch.from_numpy(img))\n",
    "        compressed_video = torch.stack(compressed_video)\n",
    "    return compressed_video\n",
    "\n",
    "compressed_video = compress_video_with_ffmpeg(vid_w, fps=fps, codec='h264', crf=18)\n",
    "print(vid_w.shape)\n",
    "print(compressed_video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codecs:\n",
    "#  D..... = Decoding supported\n",
    "#  .E.... = Encoding supported\n",
    "#  ..V... = Video codec\n",
    "#  ..A... = Audio codec\n",
    "#  ..S... = Subtitle codec\n",
    "#  ...I.. = Intra frame-only codec\n",
    "#  ....L. = Lossy compression\n",
    "#  .....S = Lossless compression\n",
    "# !ffmpeg -codecs\n",
    "!/bin/ffmpeg -codecs | grep 'EV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codecs:\n",
    "#  D..... = Decoding supported\n",
    "#  .E.... = Encoding supported\n",
    "#  ..V... = Video codec\n",
    "#  ..A... = Audio codec\n",
    "#  ..S... = Subtitle codec\n",
    "#  ...I.. = Intra frame-only codec\n",
    "#  ....L. = Lossy compression\n",
    "#  .....S = Lossless compression\n",
    "# !ffmpeg -codecs\n",
    "!/bin/ffmpeg -codecs | grep 'EV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablesign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
