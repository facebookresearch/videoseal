{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Resolution inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in the root of the repository\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.utils.display import save_img\n",
    "from videoseal.utils import Timer\n",
    "from videoseal.evals.full import setup_model_from_checkpoint\n",
    "from videoseal.evals.metrics import bit_accuracy\n",
    "from videoseal.augmentation import Identity, JPEG\n",
    "from videoseal.modules.jnd import JND\n",
    "\n",
    "import os\n",
    "import omegaconf\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding watermark  - took 0.38s\n",
      "saving videos to outputs/00\n",
      "saving videos - took 1.51s\n",
      "compressing and detecting watermarks\n",
      "tensor([[ 0.0000,  0.9498,  0.9816,  0.9610,  0.9585, -0.9406, -0.9424, -0.9920,\n",
      "          0.8935,  0.9385,  0.9679, -0.9509, -0.9353, -0.9915, -0.9152, -0.9057,\n",
      "         -0.9686, -0.9503,  1.0000,  0.9371, -0.9873, -0.9805, -0.9527, -0.9509,\n",
      "         -0.9942,  0.9484, -0.9855, -0.9672,  0.9832,  0.9626,  0.9751]])\n",
      "bit accuracy at JPEG 80 is 1.00 - took 0.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|          | 0/4 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# detect\u001b[39;00m\n\u001b[1;32m     81\u001b[0m timer\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 82\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds)\n",
      "File \u001b[0;32m~/miniconda3/envs/img/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/09-videoseal/baselines/videoseal/models/video_wam.py:356\u001b[0m, in \u001b[0;36mVideoWam.detect\u001b[0;34m(self, imgs, is_video, interpolation)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(imgs), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size):\n\u001b[1;32m    355\u001b[0m     nimgs_in_ck \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size, \u001b[38;5;28mlen\u001b[39m(imgs) \u001b[38;5;241m-\u001b[39m ii)\n\u001b[0;32m--> 356\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m:\u001b[49m\u001b[43mii\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mnimgs_in_ck\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     preds \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    361\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mappend(preds)  \u001b[38;5;66;03m# n k ..\u001b[39;00m\n",
      "File \u001b[0;32m~/09-videoseal/baselines/videoseal/models/wam.py:205\u001b[0m, in \u001b[0;36mWam.detect\u001b[0;34m(self, imgs, interpolation)\u001b[0m\n\u001b[1;32m    202\u001b[0m imgs_res \u001b[38;5;241m=\u001b[39m imgs_res\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# detect watermark\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_res\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(imgs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m: preds,  \u001b[38;5;66;03m# predicted masks and/or messages: b (1+nbits) h w\u001b[39;00m\n\u001b[1;32m    209\u001b[0m }\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/img/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/img/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/09-videoseal/baselines/videoseal/models/baselines.py:192\u001b[0m, in \u001b[0;36mBaselineCINExtractor.forward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    190\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(imgs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# scale the output to be between -1,1\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m msgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# b k\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# add +1 to the last dimension to make it b k+1 (WAM compatible)\u001b[39;00m\n\u001b[1;32m    194\u001b[0m msgs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mzeros(msgs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(msgs\u001b[38;5;241m.\u001b[39mdevice), msgs], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/img/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/img/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Directory containing videos\n",
    "assets_dir = \"/checkpoint/pfz/projects/videoseal/assets/imgs\"\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Checkpoint\n",
    "ckpts = {\n",
    "    # \"hidden\": '/private/home/hadyelsahar/work/code/videoseal/2024_logs/videoseal0.1/_lambda_d=0.5_lambda_i=0.0_optimizer=AdamW,lr=1e-4_videowam_step_size=4_video_start=500_embedder_model=hidden/checkpoint.pth',\n",
    "    # \"unet\": '/private/home/hadyelsahar/work/code/videoseal/2024_logs/videoseal0.1/_lambda_d=0.5_lambda_i=0.5_optimizer=AdamW,lr=1e-4_videowam_step_size=4_video_start=500_embedder_model=unet_small2/checkpoint.pth',\n",
    "    # \"scaling_laws_smalldetector_tinyembedder\":\"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1105-videoseal0.2-scalinglaws/_lambda_d=0.0_extractor_model=sam_small_embedder_model=0/checkpoint.pth\",\n",
    "    # \"scaling_laws_tinydetector_tinyembedder\":\"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1105-videoseal0.2-scalinglaws/_lambda_d=0.0_extractor_model=sam_tiny_embedder_model=0/checkpoint.pth\",\n",
    "    # \"JND_fix_discloss\":\"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1108-videoseal0.2-discloss-fix-removeunused-params/_attenuation=jnd_3_3_nbits=64_lambda_d=0.5_video_start=100/checkpoint.pth\",\n",
    "    # \"1111_discloss_sleepwake_highssim\":\"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1109-videoseal0.2-discloss-fix-hing-sleepwake-4nodes/_scaling_w=0.1_lambda_i=0.25_disc_hinge_on_logits_fake=False_sleepwake=True_video_start=500/checkpoint.pth\"\n",
    "    # \"1111-finetuned\":\"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1111-videoseal0.2-sleepwake-resume/_attenuation=jnd_1_1/checkpoint.pth\"\n",
    "    # \"1112-videoseal0.2\":\"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1111-videoseal0.2-archsearch-4nodes/_attenuation=None_nbits=64_finetune_detector_start=1000_embedder_model=unet_small2_quant/checkpoint.pth\",\n",
    "    # \"1118-yuv-400\":\"/checkpoint/pfz/2024_logs/1118_vseal_long_sab/_scheduler=0_optimizer=adopt,lr=1e-5/checkpoint400.pth\",\n",
    "    # \"1118-yuv-800\":\"/checkpoint/pfz/2024_logs/1118_vseal_long_sab/_scheduler=1_optimizer=AdamW,lr=1e-5/checkpoint.pth\",\n",
    "    # \"trustmark\": \"baseline/trustmark\",\n",
    "    # \"videoseal0.2b\": \"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1111-videoseal0.2-archsearch-4nodes/_attenuation=None_nbits=64_finetune_detector_start=800_embedder_model=unet_small2_quant/checkpoint.pth\",\n",
    "    # \"videoseal0.2a\": \"/private/home/hadyelsahar/work/code/videoseal/2024_logs_large-exp/1109-videoseal0.2-discloss-fix-hing-sleepwake-4nodes/_scaling_w=0.5_lambda_i=0.5_disc_hinge_on_logits_fake=True_sleepwake=False_video_start=500/checkpoint.pth\",\n",
    "    # \"videoseal0.4\": \"/large_experiments/meres/hadyelsahar/2024_logs/1120-videoseal0.4/_scaling_w=0.5_sleepwake=False_videowam_step_size=4_extractor_model=sam_tiny/checkpoint.pth\",\n",
    "    # \"wam\": \"baseline/wam\",\n",
    "    \"cin\": \"baseline/cin\",\n",
    "    # \"mbrs\": \"baseline/mbrs\",\n",
    "}\n",
    "\n",
    "for ckpt_name, ckpt_path in ckpts.items():\n",
    "    # ckpt_path = \"/checkpoint/pfz/2024_logs/1028_vseal_long/_seed=3_optimizer=AdamW,lr=1e-4_embedder_model=unet_small2_yuv/checkpoint600.pth\"\n",
    "\n",
    "    # a timer to measure the time\n",
    "    timer = Timer()\n",
    "\n",
    "    # Iterate over all checkpoints\n",
    "    wam = setup_model_from_checkpoint(ckpt_path)\n",
    "    wam.eval()\n",
    "    wam.to(device)\n",
    "\n",
    "    # Iterate over all video files in the directory\n",
    "    files = [f for f in os.listdir(assets_dir) if f.endswith(\".png\")]\n",
    "    files = [os.path.join(assets_dir, f) for f in files]\n",
    "\n",
    "    for file in tqdm(files, desc=f\"Processing Images\"):\n",
    "        # load image\n",
    "        imgs = Image.open(file, \"r\").convert(\"RGB\")  # keep only rgb channels\n",
    "        imgs = to_tensor(imgs).unsqueeze(0).float()\n",
    "\n",
    "        # Watermark embedding\n",
    "        timer.start()\n",
    "        outputs = wam.embed(imgs, is_video=False)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"embedding watermark  - took {timer.stop():.2f}s\")\n",
    "\n",
    "        # compute diff\n",
    "        imgs_w = outputs[\"imgs_w\"]  # b c h w\n",
    "        msgs = outputs[\"msgs\"]  # b k\n",
    "        diff = imgs_w - imgs\n",
    "\n",
    "        # save\n",
    "        timer.start()\n",
    "        base_save_name = os.path.join(output_dir, os.path.basename(file).replace(\".png\", \"\"))\n",
    "        print(f\"saving videos to {base_save_name}\")\n",
    "        save_img(imgs[0], f\"{base_save_name}_ori.png\")\n",
    "        save_img(imgs_w[0], f\"{base_save_name}_wm.png\")\n",
    "        save_img(10*diff[0].abs(), f\"{base_save_name}_diff.png\")\n",
    "\n",
    "        # Compute min and max values, reshape, and normalize\n",
    "        min_vals = diff.view(imgs.shape[0], imgs.shape[1], -1).min(dim=2, keepdim=True)[0].view(imgs.shape[0], imgs.shape[1], 1, 1)\n",
    "        max_vals = diff.view(imgs.shape[0], imgs.shape[1], -1).max(dim=2, keepdim=True)[0].view(imgs.shape[0], imgs.shape[1], 1, 1)\n",
    "        normalized_images = (diff - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "        # Save the normalized video\n",
    "        save_img(normalized_images[0], f\"{base_save_name}_diff_norm.png\")\n",
    "        print(f\"saving videos - took {timer.stop():.2f}s\")\n",
    "\n",
    "        # Augment video\n",
    "        print(f\"compressing and detecting watermarks\")\n",
    "        for qf in [80, 40]:\n",
    "            imgs_aug, _ = JPEG()(imgs_w, None,qf)\n",
    "\n",
    "            # detect\n",
    "            timer.start()\n",
    "            outputs = wam.detect(imgs_aug, is_video=True)\n",
    "            preds = outputs[\"preds\"]\n",
    "            print(preds)\n",
    "            bit_preds = preds[:, 1:]  # b k ...\n",
    "            bit_accuracy_ = bit_accuracy(\n",
    "                bit_preds,\n",
    "                msgs\n",
    "            ).nanmean().item()\n",
    "            print(f\"bit accuracy at JPEG {qf} is {bit_accuracy_:.2f} - took {timer.stop():.2f}s\")\n",
    "\n",
    "        del outputs, imgs, imgs_w, diff, min_vals, max_vals, normalized_images\n",
    "\n",
    "    # Free model from GPU\n",
    "    del wam\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With attenuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing videos\n",
    "assets_dir = \"assets/imgs\"\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Checkpoint\n",
    "ckpt_path = \"/checkpoint/pfz/2024_logs/1028_vseal_long/_seed=3_optimizer=AdamW,lr=1e-4_embedder_model=unet_small2_yuv/checkpoint600.pth\"\n",
    "\n",
    "# a timer to measure the time\n",
    "timer = Timer()\n",
    "\n",
    "# Iterate over all checkpoints\n",
    "wam = setup_model_from_checkpoint(ckpt_path)\n",
    "wam.eval()\n",
    "wam.to(device)\n",
    "\n",
    "# create attenuation\n",
    "attenuation_cfg = \"configs/attenuation.yaml\"\n",
    "attenuation = \"jnd_1_1\"\n",
    "attenuation_cfg = omegaconf.OmegaConf.load(attenuation_cfg)[attenuation]\n",
    "attenuation = JND(**attenuation_cfg)\n",
    "wam.attenuation = attenuation\n",
    "wam.scaling_w = 0.2\n",
    "\n",
    "# Iterate over all video files in the directory\n",
    "files = [f for f in os.listdir(assets_dir) if f.endswith(\".png\")]\n",
    "files = [os.path.join(assets_dir, f) for f in files]\n",
    "\n",
    "for file in tqdm(files, desc=f\"Processing Videos\"):\n",
    "    # load image\n",
    "    imgs = Image.open(file, \"r\").convert(\"RGB\")  # keep only rgb channels\n",
    "    imgs = to_tensor(imgs).unsqueeze(0).float()\n",
    "\n",
    "    # Watermark embedding\n",
    "    timer.start()\n",
    "    outputs = wam.embed(imgs, is_video=False)\n",
    "    # torch.cuda.synchronize()\n",
    "    # print(f\"embedding watermark  - took {timer.stop():.2f}s\")\n",
    "\n",
    "    # compute diff\n",
    "    imgs_w = outputs[\"imgs_w\"]  # b c h w\n",
    "    msgs = outputs[\"msgs\"]  # b k\n",
    "    diff = imgs_w - imgs\n",
    "\n",
    "    # save\n",
    "    timer.start()\n",
    "    base_save_name = os.path.join(output_dir, os.path.basename(file).replace(\".png\", \"\"))\n",
    "    save_img(imgs[0], f\"{base_save_name}_ori.png\")\n",
    "    save_img(imgs_w[0], f\"{base_save_name}_wm.png\")\n",
    "    save_img(diff[0], f\"{base_save_name}_diff.png\")\n",
    "\n",
    "    # Compute min and max values, reshape, and normalize\n",
    "    min_vals = diff.view(imgs.shape[0], imgs.shape[1], -1).min(dim=2, keepdim=True)[0].view(imgs.shape[0], imgs.shape[1], 1, 1)\n",
    "    max_vals = diff.view(imgs.shape[0], imgs.shape[1], -1).max(dim=2, keepdim=True)[0].view(imgs.shape[0], imgs.shape[1], 1, 1)\n",
    "    normalized_images = (diff - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "    # Save the normalized video\n",
    "    save_img(normalized_images[0], f\"{base_save_name}_diff_norm.png\")\n",
    "\n",
    "    # Augment video\n",
    "    for qf in [80, 40]:\n",
    "        imgs_aug, _ = JPEG()(imgs_w, None,qf)\n",
    "\n",
    "        # detect\n",
    "        timer.start()\n",
    "        outputs = wam.detect(imgs_aug, is_video=True)\n",
    "        preds = outputs[\"preds\"]\n",
    "        bit_preds = preds[:, 1:]  # b k ...\n",
    "        bit_accuracy_ = bit_accuracy(\n",
    "            bit_preds,\n",
    "            msgs\n",
    "        ).nanmean().item()\n",
    "        print(f\"bit accuracy at JPEG {qf} is {bit_accuracy_:.2f} - took {timer.stop():.2f}s\")\n",
    "\n",
    "    del outputs, imgs, imgs_w, diff, min_vals, max_vals, normalized_images\n",
    "\n",
    "# Free model from GPU\n",
    "del wam\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
