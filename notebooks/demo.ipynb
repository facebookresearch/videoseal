{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pfz/Code/05-videowm/videoseal\n"
     ]
    }
   ],
   "source": [
    "# Run in the root of the repository\n",
    "%cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/miniconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import logging\n",
    "logging.getLogger(\"matplotlib.image\").setLevel(logging.ERROR)\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import videoseal\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fps = 24 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the VideoSeal Model\n",
    "The videoseal library provides pretrained models for embedding and extracting watermarks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /Users/pfz/.cache/huggingface/hub/models--facebook--videoseal/snapshots/8ad5f54cabb89a0d3e6591f06d0ac90f12bca7e1/checkpoint.pth with message: <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Videoseal(\n",
       "  (embedder): UnetEmbedder(\n",
       "    (unet): UNetMsg(\n",
       "      (msg_processor): MsgProcessor(\n",
       "        (msg_embeddings): Embedding(192, 192)\n",
       "      )\n",
       "      (inc): ResnetBlock(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): ChanRMSNorm()\n",
       "          (2): SiLU()\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): ChanRMSNorm()\n",
       "          (5): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (downs): ModuleList(\n",
       "        (0): DBlock(\n",
       "          (down): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): DBlock(\n",
       "          (down): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): DBlock(\n",
       "          (down): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bottleneck): BottleNeck(\n",
       "        (model): Sequential(\n",
       "          (0): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (4): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (6): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (7): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(640, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): UBlock(\n",
       "          (up): Upsample(\n",
       "            (upsample_block): Sequential(\n",
       "              (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "              (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "              (2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "              (3): LayerNorm()\n",
       "              (4): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv): ResnetBlock(\n",
       "            (double_conv): Sequential(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): ChanRMSNorm()\n",
       "              (2): SiLU()\n",
       "              (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (4): ChanRMSNorm()\n",
       "              (5): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (outc): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (msg_processor): MsgProcessor(\n",
       "      (msg_embeddings): Embedding(192, 192)\n",
       "    )\n",
       "  )\n",
       "  (detector): SegmentationExtractor(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (lin2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm()\n",
       "        (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (pixel_decoder): PixelDecoder(\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): Upsample(\n",
       "          (upsample_block): Sequential(\n",
       "            (0): Upsample(scale_factor=1.0, mode='bilinear')\n",
       "            (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "            (2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "            (3): LayerNorm()\n",
       "            (4): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (linear): Linear(in_features=384, out_features=97, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (augmenter): Augmenter(augs=['Identity'], probs=tensor([1.]))\n",
       "  (rgb2yuv): RGB2YUV()\n",
       "  (blender): Blender()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the VideoSeal model\n",
    "model = videoseal.load(\"videoseal\")\n",
    "\n",
    "# Set the model to evaluation mode and move it to the selected device\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Video\n",
    "We will process a single video. For demonstration, the video is trimmed to the first 3 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "# Path to the input video\n",
    "video_path = \"./assets/videos/1.mp4\"\n",
    "\n",
    "# Read the video and convert to tensor format\n",
    "video, _, _ = torchvision.io.read_video(video_path, output_format=\"TCHW\")\n",
    "\n",
    "# Normalize the video frames to the range [0, 1] and trim to 1 second\n",
    "video = video[:fps * 5].float() / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videoseal Embedder: Embed the Watermark\n",
    "The model embeds a watermark into the video frames. This step generates the watermarked frames (imgs_w) and returns the random watermark message (msgs) so you know which message was embedded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform watermark embedding\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the results\u001b[39;00m\n\u001b[1;32m      5\u001b[0m video_w \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs_w\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Watermarked video frames\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/05-videowm/videoseal/videoseal/models/videoseal.py:222\u001b[0m, in \u001b[0;36mVideoseal.embed\u001b[0;34m(self, imgs, msgs, is_video)\u001b[0m\n\u001b[1;32m    219\u001b[0m     deltas_in_ck \u001b[38;5;241m=\u001b[39m deltas_in_ck[:\u001b[38;5;28mlen\u001b[39m(all_imgs_in_ck)]\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# create watermarked imgs\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     all_imgs_in_ck_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_imgs_in_ck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas_in_ck\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     imgs_w[start: end, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m all_imgs_in_ck_w  \u001b[38;5;66;03m# n 3 h w\u001b[39;00m\n\u001b[1;32m    225\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs_w\u001b[39m\u001b[38;5;124m\"\u001b[39m: imgs_w,  \u001b[38;5;66;03m# watermarked imgs: f 3 h w\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsgs\u001b[39m\u001b[38;5;124m\"\u001b[39m: msgs[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(imgs), \u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# original messages: f k\u001b[39;00m\n\u001b[1;32m    228\u001b[0m }\n",
      "File \u001b[0;32m~/Code/05-videowm/videoseal/videoseal/models/wam.py:105\u001b[0m, in \u001b[0;36mWam.blend\u001b[0;34m(self, imgs, preds_w)\u001b[0m\n\u001b[1;32m     99\u001b[0m     preds_w \u001b[38;5;241m=\u001b[39m preds_w\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# # or equivalently\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# imgs_w = rgb_to_yuv(imgs)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# imgs_w[:, 0:1] = self.scaling_i * imgs_w[:, 0:1] + self.scaling_w * preds_w\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# imgs_w = yuv_to_rgb(imgs_w)\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m imgs_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m imgs_w\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/05-videowm/videoseal/videoseal/models/blender.py:65\u001b[0m, in \u001b[0;36mBlender.forward\u001b[0;34m(self, imgs, preds_w)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Clamp output if specified\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclamp:\n\u001b[0;32m---> 65\u001b[0m     blended_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblended_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blended_output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform watermark embedding\n",
    "outputs = model.embed(video, is_video=True)\n",
    "\n",
    "# Extract the results\n",
    "video_w = outputs[\"imgs_w\"]  # Watermarked video frames\n",
    "msgs = outputs[\"msgs\"]      # Watermark messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference between watermarked and original frames\n",
    "diff = video - video_w\n",
    "\n",
    "# Normalize the difference for visualization\n",
    "min_vals = diff.view(diff.shape[0], diff.shape[1], -1).min(dim=2, keepdim=True)[0].view(diff.shape[0], diff.shape[1], 1, 1)\n",
    "max_vals = diff.view(diff.shape[0], diff.shape[1], -1).max(dim=2, keepdim=True)[0].view(diff.shape[0], diff.shape[1], 1, 1)\n",
    "normalized_images = (diff - min_vals) / (max_vals - min_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display The Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_videos_side_by_side(videos, titles):\n",
    "    \"\"\"\n",
    "    Display multiple videos side by side in a table format in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "    - videos: List of video tensors (torch.Tensor).\n",
    "    - titles: List of column titles corresponding to the videos.\n",
    "    \"\"\"\n",
    "    # Prepare video HTML strings\n",
    "    video_htmls = []\n",
    "    for video in videos:\n",
    "\n",
    "        video = video.permute(0, 2, 3, 1).cpu().numpy()  # Convert to numpy format\n",
    "                \n",
    "        # resize and display 2 secs only \n",
    "        video = video[:fps*2]\n",
    "\n",
    "        _, C, H, W = video.shape\n",
    "        new_H, new_W = 512, int((512 / H) * W)\n",
    "        new_H, new_W = (new_H // 2) * 2, (new_W // 2) * 2  # Ensure even dimensions\n",
    "        # Resize the video\n",
    "        video = F.resize(video, (new_H, new_W))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        img = ax.imshow(video_np[0])  # Display the first frame\n",
    "\n",
    "        # Remove axis ticks and labels\n",
    "        ax.axis('off')\n",
    "\n",
    "        def update(frame):\n",
    "            img.set_data(video_np[frame])\n",
    "            return img,\n",
    "\n",
    "        ani = FuncAnimation(fig, update, frames=len(video_np), interval=1000 // fps)\n",
    "        plt.close(fig)\n",
    "        video_htmls.append(ani.to_jshtml())\n",
    "    \n",
    "    # Create table with titles as headers and videos as content\n",
    "    table_html = '<table style=\"width:100%; text-align:center; border-collapse: collapse;\">'\n",
    "    table_html += '<tr>'  # Row for headers\n",
    "    for title in titles:\n",
    "        table_html += f'<th style=\"border: 1px solid black; padding: 5px;\">{title}</th>'\n",
    "    table_html += '</tr>'\n",
    "    table_html += '<tr>'  # Row for videos\n",
    "    for video_html in video_htmls:\n",
    "        table_html += f'<td style=\"border: 1px solid black; padding: 5px;\">{video_html}</td>'\n",
    "    table_html += '</tr>'\n",
    "    table_html += '</table>'\n",
    "    \n",
    "    return HTML(table_html)\n",
    "\n",
    "# Display the videos side by side\n",
    "videos = [video, video_w, normalized_images]\n",
    "titles = [\"Original Video\", \"Watermarked Video\", \"Watermark\"]\n",
    "display(display_videos_side_by_side(videos, titles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videoseal Extractor: Extract the Watermark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoseal.evals.metrics import bit_accuracy\n",
    "\n",
    "msg_extracted = model.extract_message(video_w)\n",
    "\n",
    "bit_accuracy_ = bit_accuracy(msg_extracted, msgs).nanmean().item()\n",
    "print(f\"Bit Accuracy: {bit_accuracy_:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test VideoSeal Robustness to H264 Codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from videoseal.augmentation import H264\n",
    "\n",
    "def compress_and_eval(video_w, msgs, compression_levels):\n",
    "    \"\"\"\n",
    "    Test the watermark resistance to H.264 compression.\n",
    "    \n",
    "    Args:\n",
    "    - video_w (torch.Tensor): Watermarked video frames.\n",
    "    - msgs (torch.Tensor): Original watermark messages.\n",
    "    - compression_levels (list): List of CRF values for compression.\n",
    "    \n",
    "    Returns:\n",
    "    - results (list): List of dictionaries containing CRF, Bit Accuracy, and Detection Time.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    h264 = H264()  # H.264 compression augmentation\n",
    "\n",
    "    for crf in compression_levels:\n",
    "        # Apply H.264 compression\n",
    "        video_compressed, _ = h264(video_w, crf=crf)\n",
    "\n",
    "        # Detect watermarks in the compressed video        \n",
    "        outputs = model.detect(video_compressed, is_video=True)\n",
    "\n",
    "        # Compute bit accuracy\n",
    "        preds = outputs[\"preds\"]\n",
    "        bit_preds = preds[:, 1:]  # Extract watermark bits\n",
    "        bit_acc = bit_accuracy(bit_preds, msgs).nanmean().item()\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"CRF\": crf,\n",
    "            \"Bit Accuracy\": round(bit_acc, 3),\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test the watermark resistance at different CRF levels\n",
    "compression_levels = [15, 23, 30, 32, 35, 38, 40, 60]  # CRF values for H.264 compression\n",
    "results = compress_and_eval(video_w, msgs, compression_levels)\n",
    "\n",
    "# Display results in a table\n",
    "df_results = pd.DataFrame(results)\n",
    "display(HTML(\"<h3>Watermark Resistance to Compression</h3>\"))\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
